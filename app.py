from flask import Flask, jsonify, request, render_template
from flask_socketio import SocketIO, emit
import os
import random
import math  # Add math module for UCT calculations

# Load environment variables from .env file
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    # dotenv not installed, will use environment variables directly
    pass
from src.mcts.node import MCTSState, MCTSNode
from src.mcts.tree import MCTS
from pathlib import Path
from werkzeug.utils import secure_filename
import uuid
from src.agents.structured_review import StructuredReviewAgent
from src.agents.ideation import IdeationAgent
from src.agents.review import ReviewAgent
from src.utils.ib_config import load_physics_topics, load_chemistry_topics, load_topics_for_subject, validate_rq, load_rq_requirements
from src.agents.prompts import validate_rq_format
import json
import re
import yaml
import traceback
import sys
import logging
import click
import requests

logger = logging.getLogger(__name__)

# Remove the old external path and use local scholarqa package
sys.path.insert(0, str(Path(__file__).parent / "src" / "retrieval_api"))
from scholarqa import ScholarQA
from scholarqa.rag.retrieval import PaperFinder, PaperFinderWithReranker
from scholarqa.rag.retriever_base import FullTextRetriever
# NOTE: HuggingFace reranker imports are done lazily to avoid loading torch/transformers
# on Railway where RERANK_MODE=none (default). This keeps the Docker image under 4GB.
# See reranker initialization below for lazy import logic.
import pymupdf  # PyMuPDF for PDF parsing
# Import the key manager
# from src.utils.key_manager import encrypt_api_key, decrypt_api_key, get_client_encryption_script

app = Flask(__name__)
app.config["UPLOAD_FOLDER"] = "uploads"
app.config["MAX_CONTENT_LENGTH"] = 16 * 1024 * 1024  # 16MB max file size
app.config['SECRET_KEY'] = os.environ.get('FLASK_SECRET_KEY', 'secret!')
socketio = SocketIO(app)

# Create uploads directory if it doesn't exist
os.makedirs(app.config["UPLOAD_FOLDER"], exist_ok=True)

# Create a secure key directory for storing encrypted keys if not exists
SECURE_KEYS_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'secure_keys')
os.makedirs(SECURE_KEYS_DIR, exist_ok=True)

# Initialize knowledge storage
knowledge_chunks = []

# Initialize empty storage
chat_messages = []
main_idea = "Generating Research Idea..."
retrieval_results = {}

def allowed_file(filename):
    """Check if the file extension is allowed"""
    # Get file extension (last part after the dot)
    extension = filename.rsplit(".", 1)[1].lower() if "." in filename else ""
    return extension in {
        "txt",
        "pdf",
        "doc",
        "docx",
    }

# Function to safely load API keys from environment or config file
def get_api_key(key_name, config_dict):
    """Get API key from environment variable with preference to environment variable"""
    env_var_name = f"{key_name.upper()}_API_KEY"
    env_key = os.environ.get(env_var_name)
    
    if env_key:
        return env_key
    
    # Never return sensitive keys from config in production
    if os.environ.get('FLASK_ENV') == 'production':
        return None
        
    # Only fall back to config file in development
    if "keys" in config_dict and key_name in config_dict["keys"]:
        return config_dict["keys"][key_name]
    
    return None

def extract_abstract(pdf_text):
    full_text = "\n".join(pdf_text)
    match = re.search(r"(Abstract.*?)(\n(?:Introduction|Keywords|1\.)|\n{2,})", full_text, re.DOTALL | re.IGNORECASE)
    if match:
        return match.group(1).strip()
    return "No abstract found."

# Initialize MCTS
mcts = MCTS("config/config.yaml")
current_root = None
current_node = None
selected_subject = None  # Selected subject (Physics, Chemistry, etc.)
current_state = None
exploration_in_progress = False

# Initialize agents
structured_review_agent = StructuredReviewAgent("config/config.yaml")
ideation_agent = IdeationAgent("config/config.yaml")
review_agent = ReviewAgent("config/config.yaml")  # Initialize review agent directly

# Get the configuration from config.yaml
import yaml
with open("config/config.yaml", "r") as f:
    config = yaml.safe_load(f)

# Set Semantic Scholar API key
s2_api_key = os.environ.get("SEMANTIC_SCHOLAR_API_KEY")
if not s2_api_key:
    logger.warning("SEMANTIC_SCHOLAR_API_KEY environment variable is not set. Semantic Scholar API features will be disabled.")
    os.environ['SEMANTIC_SCHOLAR_API_KEY'] = ''
else:
    os.environ['SEMANTIC_SCHOLAR_API_KEY'] = s2_api_key

# Set Google API key securely only if not in Azure OpenAI deployment mode
deploy_mode = os.environ.get('DEPLOY', 'false').lower() == 'true'
if not deploy_mode:
    gemini_key = get_api_key("googleaistudio", config)
    if gemini_key:
        os.environ['GEMINI_API_KEY'] = gemini_key
else:
    # In Azure OpenAI mode, ensure Azure OpenAI keys are available
    azure_key = os.environ.get("AZURE_OPENAI_API_KEY")
    azure_endpoint = os.environ.get("AZURE_OPENAI_ENDPOINT")
    if not azure_key:
        logger.warning("AZURE_OPENAI_API_KEY not set while in deployment mode")
    if not azure_endpoint:
        logger.warning("AZURE_OPENAI_ENDPOINT not set while in deployment mode")

# =============================================================================
# RERANKER CONFIGURATION
# =============================================================================
# RERANK_MODE controls whether to use HuggingFace reranker:
#   - "hf"   : Use HuggingFace cross-encoder reranker (requires torch/transformers - LOCAL DEV ONLY)
#   - "none" : No reranking, use retrieval scores only (PRODUCTION DEFAULT)
#
# On Railway: Leave RERANK_MODE unset or set to "none" to avoid loading torch (~2GB+)
# Locally: Set RERANK_MODE=hf in .env to enable reranking
# =============================================================================

retriever = FullTextRetriever(n_retrieval=20, n_keyword_srch=20)

# Determine rerank mode from environment
# Default: "none" (no reranking) to keep Railway image small
rerank_mode = os.environ.get("RERANK_MODE", "none").lower()

if rerank_mode == "hf":
    # LAZY IMPORT: Only import HuggingFace reranker when explicitly enabled
    # This prevents torch/transformers from being loaded on Railway
    try:
        from scholarqa.rag.reranker.modal_engine import HuggingFaceReranker
        reranker = HuggingFaceReranker(model_name="cross-encoder/ms-marco-MiniLM-L-6-v2", batch_size=256)
        paper_finder = PaperFinderWithReranker(retriever, reranker=reranker, n_rerank=10, context_threshold=0.1)
        logger.info("Using HuggingFace reranker (RERANK_MODE=hf)")
    except ImportError as e:
        logger.warning(f"HuggingFace reranker not available (missing torch/transformers): {e}")
        logger.warning("Falling back to no reranking. Install requirements-dev.txt for local reranking.")
        paper_finder = PaperFinder(retriever, context_threshold=0.1)
else:
    # Default: No reranking (keeps Railway image under 4GB)
    paper_finder = PaperFinder(retriever, context_threshold=0.1)
    logger.info("Reranking disabled (RERANK_MODE=none or unset)")

scholar_qa = ScholarQA(paper_finder=paper_finder, llm_model="gemini/gemini-2.0-flash-lite")

# API Key management endpoints with improved security
@app.route("/api/set_api_key", methods=["POST"])
def set_api_key():
    """Save an API key securely"""
    try:
        # Verify request contains required data
        data = request.get_json()
        if not data or "provider" not in data or "key" not in data:
            return jsonify({"error": "Missing required fields"}), 400
            
        provider = data["provider"]
        api_key = data["key"]
        
        # Validate API key format based on provider
        key_formats = {
            "openai": r"^sk-[A-Za-z0-9]{48}$",
            "claude": r"^sk-ant-[A-Za-z0-9]{32,}$",
            "deepseek": r"^[A-Za-z0-9]{32,}$",
            "gemini": r"^AIza[A-Za-z0-9_-]{35}$",
            "semantic_scholar": r"^[A-Za-z0-9]{40}$"
        }
        
        if provider in key_formats:
            if not re.match(key_formats[provider], api_key):
                return jsonify({"error": f"Invalid {provider} API key format"}), 400
                
        # Encrypt the API key if not in local development mode
        is_production = os.environ.get('FLASK_ENV') == 'production'
        
        # Read the current config for local development
        with open("config/config.yaml", "r") as f:
            config = yaml.safe_load(f)
            
        # Ensure the keys section exists
        if "keys" not in config:
            config["keys"] = {}
            
        # Set environment variable immediately for current session
        provider_key_mapping = {
            "openai": "OPENAI_API_KEY",
            "claude": "ANTHROPIC_API_KEY",
            "deepseek": "DEEPSEEK_API_KEY",
            "gemini": "GEMINI_API_KEY",
            "semantic_scholar": "SEMANTIC_SCHOLAR_API_KEY"
        }
        
        # Save the key to environment variable for immediate use
        if provider in provider_key_mapping:
            os.environ[provider_key_mapping[provider]] = api_key
        
        # In production, encrypt keys for storage
        if is_production:
            # Use app secret key as encryption password
            encrypted_data = encrypt_api_key(api_key, app.config['SECRET_KEY'])
            
            # Store encrypted key by provider
            encrypted_key_path = os.path.join(SECURE_KEYS_DIR, f"{provider}.json")
            with open(encrypted_key_path, 'w') as f:
                json.dump(encrypted_data, f)
                
            return jsonify({
                "status": "success", 
                "message": f"API key for {provider} saved securely"
            })
        else:
            # For local development, save in config file
            if provider == "openai":
                config["keys"]["openai_key"] = api_key
            elif provider == "claude":
                config["keys"]["anthropic_key"] = api_key
            elif provider == "deepseek":
                config["keys"]["deepseek_key"] = api_key
            elif provider == "gemini":
                config["keys"]["googleaistudio_key"] = api_key
                
            # Save the updated config
            with open("config/config.yaml", "w") as f:
                yaml.dump(config, f, default_flow_style=False)
                
            return jsonify({
                "status": "success", 
                "message": f"API key for {provider} saved successfully"
            })
        
    except Exception as e:
        error_message = f"Error saving API key: {str(e)}"
        print(error_message)
        return jsonify({"error": error_message}), 500

@app.route("/api/delete_api_key", methods=["POST"])
def delete_api_key():
    """Delete an API key"""
    try:
        data = request.get_json()
        if not data or "provider" not in data:
            return jsonify({"error": "Missing provider field"}), 400
            
        provider = data["provider"]
        is_production = os.environ.get('FLASK_ENV') == 'production'
        
        if is_production:
            # For production, remove the encrypted key file
            encrypted_key_path = os.path.join(SECURE_KEYS_DIR, f"{provider}.json")
            if os.path.exists(encrypted_key_path):
                os.remove(encrypted_key_path)
                return jsonify({
                    "status": "success", 
                    "message": f"API key for {provider} deleted successfully"
                })
            else:
                return jsonify({
                    "status": "error", 
                    "message": f"No API key found for {provider}"
                }), 404
        else:
            # For local development, remove from config file
            with open("config/config.yaml", "r") as f:
                config = yaml.safe_load(f)
                
            # Check if keys section exists
            if "keys" not in config:
                return jsonify({"status": "error", "message": "No API keys found"}), 404
                
            # Remove appropriate key based on provider
            key_removed = False
            if provider == "openai" and "openai_key" in config["keys"]:
                del config["keys"]["openai_key"]
                key_removed = True
            elif provider == "claude" and "anthropic_key" in config["keys"]:
                del config["keys"]["anthropic_key"]
                key_removed = True
            elif provider == "deepseek" and "deepseek_key" in config["keys"]:
                del config["keys"]["deepseek_key"]
                key_removed = True
            elif provider == "gemini" and "googleaistudio_key" in config["keys"]:
                del config["keys"]["googleaistudio_key"]
                key_removed = True
                
            if key_removed:
                # Save the updated config
                with open("config/config.yaml", "w") as f:
                    yaml.dump(config, f, default_flow_style=False)
                return jsonify({
                    "status": "success", 
                    "message": f"API key for {provider} deleted successfully"
                })
            else:
                return jsonify({
                    "status": "error", 
                    "message": f"No API key found for {provider}"
                }), 404
            
    except Exception as e:
        error_message = f"Error deleting API key: {str(e)}"
        print(error_message)
        return jsonify({"error": error_message}), 500

@app.route("/api/get_api_keys", methods=["GET"])
def get_api_keys():
    """Get a list of configured API keys (without exposing the actual key values)"""
    try:
        is_production = os.environ.get('FLASK_ENV') == 'production'
        configured_keys = []
        
        if is_production:
            # In production, check for encrypted key files
            for provider in ["openai", "claude", "deepseek", "gemini"]:
                encrypted_key_path = os.path.join(SECURE_KEYS_DIR, f"{provider}.json")
                if os.path.exists(encrypted_key_path):
                    configured_keys.append({
                        "provider": provider,
                        "configured": True
                    })
        else:
            # In local development, check the config file
            with open("config/config.yaml", "r") as f:
                config = yaml.safe_load(f)
                
            # Check if keys section exists
            if "keys" in config:
                # Create a list of configured providers (without exposing the actual keys)
                key_mapping = {
                    "openai_key": "openai",
                    "anthropic_key": "claude", 
                    "deepseek_key": "deepseek",
                    "googleaistudio_key": "gemini"
                }
                
                for key_name, provider_name in key_mapping.items():
                    if key_name in config["keys"] and config["keys"][key_name]:
                        configured_keys.append({
                            "provider": provider_name,
                            "configured": True
                        })
                
        return jsonify({"keys": configured_keys})
        
    except Exception as e:
        error_message = f"Error retrieving API keys: {str(e)}"
        print(error_message)
        return jsonify({"error": error_message}), 500

# Endpoint to provide client-side encryption code
@app.route("/api/encryption-script")
def get_encryption_script():
    """Return JavaScript code for client-side encryption"""
    return get_client_encryption_script(), 200, {'Content-Type': 'application/javascript'}

@app.route("/api/llm_client_info")
def get_llm_client_info():
    """Get information about which LLM client is being used"""
    try:
        from src.agents.llm_utils import get_client_info
        client_info = get_client_info()
        
        # Add environment variable info
        env_info = {
            "DEPLOY": os.environ.get('DEPLOY', 'false'),
            "AZURE_OPENAI_API_KEY_SET": bool(os.environ.get('AZURE_OPENAI_API_KEY')),
            "AZURE_OPENAI_ENDPOINT_SET": bool(os.environ.get('AZURE_OPENAI_ENDPOINT')),
            "AZURE_OPENAI_API_VERSION": os.environ.get('AZURE_OPENAI_API_VERSION', '2024-06-01')
        }
        
        return jsonify({
            "client_info": client_info,
            "environment": env_info
        })
        
    except Exception as e:
        return jsonify({"error": f"Failed to get client info: {str(e)}"}), 500

def get_latest_feedback(feedback_dict):
    """Extract the most recent feedback message from feedback dictionary"""
    if not feedback_dict or not isinstance(feedback_dict, dict):
        return None
    if len(feedback_dict) == 0:
        return None
    # Sort by timestamp (key) and get most recent
    sorted_entries = sorted(feedback_dict.items(), reverse=True)
    return sorted_entries[0][1]  # Return the message (value)


@app.route("/")
def index():
    return render_template("index.html")


@app.route("/api/knowledge", methods=["GET"])
def get_knowledge():
    return jsonify(knowledge_chunks)


@app.route("/api/add_knowledge", methods=["POST"])
def add_knowledge():
    data = request.get_json()
    if not data or "text" not in data or "source" not in data:
        return jsonify({"error": "Invalid payload"}), 400
    new_id = len(knowledge_chunks) + 1
    chunk = {
        "id": new_id,
        "text": data["text"],
        "full_text": data["text"],
        "source": data["source"],
    }
    knowledge_chunks.append(chunk)
    return jsonify(chunk), 201


@app.route("/api/subject", methods=["GET", "POST"])
def subject():
    """Get or set the selected subject."""
    global selected_subject
    
    if request.method == "GET":
        return jsonify({"subject": selected_subject})
    
    # POST: Set subject
    data = request.get_json()
    if not data or "subject" not in data:
        return jsonify({"error": "Invalid payload"}), 400
    
    subject_value = data.get("subject")
    # Validate subject (allow None, "physics", "chemistry", or other valid subjects)
    valid_subjects = [None, "physics", "chemistry", "default"]
    if subject_value and subject_value.lower() not in [s for s in valid_subjects if s]:
        # Allow any subject but normalize to lowercase
        subject_value = subject_value.lower() if subject_value else None
    else:
        subject_value = subject_value.lower() if subject_value else None
    
    selected_subject = subject_value
    return jsonify({"subject": selected_subject})


@app.route("/api/reset", methods=["POST"])
def reset_all():
    """Reset all application state for a new research project."""
    global main_idea, current_root, current_node, chat_messages, selected_subject, retrieval_results
    
    # Reset all global state
    main_idea = ""
    current_root = None
    current_node = None
    chat_messages = []
    selected_subject = None
    retrieval_results = {}
    
    logger.info("Application state reset - ready for new research project")
    return jsonify({"success": True})


@app.route("/api/chat", methods=["GET", "POST"])
def chat():
    global main_idea, current_root, current_node, current_state, exploration_in_progress, selected_subject
    if request.method == "GET":
        return jsonify(chat_messages)
    else:
        data = request.get_json()
        if not data or "content" not in data:
            return jsonify({"error": "Invalid payload"}), 400
        
        user_message = data["content"]
        # Get subject from request, fallback to global variable
        request_subject = data.get("subject")
        if request_subject:
            selected_subject = request_subject  # Update global subject from request
        
        chat_messages.append({"role": "user", "content": user_message})
        
        try:
            # First message: Initialize MCTS with research goal
            if current_root is None:
                # Store initial research goal in state
                chat_messages.append(
                    {"role": "system", "content": "Generating initial idea..."}
                )

                # Extract the abstract from the knowledge chunks (if available)
                abstract_text = ""
                for chunk in knowledge_chunks:
                    if "abstract" in chunk:
                        abstract_text = chunk["abstract"]
                        break 
                
                # Create a root state that represents just the research goal
                root_state = MCTSState(
                    research_goal=user_message,
                    current_idea=user_message,  # Root node "idea" is the research goal itself
                    retrieved_knowledge=[abstract_text],  # Pass abstract as retrieved knowledge
                    feedback={},
                    reward=0.0,
                    depth=0,
                    subject=selected_subject,
                )
                
                # Create root node with the research goal
                current_root = MCTSNode(state=root_state)
                
                # Use the ideation agent to generate the idea
                response = mcts.ideation_agent.execute_action(
                    "generate", 
                    {
                        "research_goal": user_message,
                        "current_idea": None,
                        "abstract": abstract_text,
                        "action_type": "execute",
                        "subject": selected_subject,
                    }
                )
                llm_response = response["content"]
                chat_messages.append({"role": "system", "content": "Initial idea generated by AI."})
                
                main_idea = llm_response
                
                # Create a state for the first generated idea
                first_idea_state = MCTSState(
                    research_goal=user_message,
                    current_idea=llm_response,
                    retrieved_knowledge=[],
                    feedback={},
                    reward=0.0,
                    depth=1,  # Depth 1 since it's a child of the root
                    subject=selected_subject,
                )
                
                # Get review using the unified review method
                review_data = review_agent.unified_review(llm_response, subject=selected_subject)
                avg_score = review_data.get("average_score") if review_data else None
                if avg_score is not None:
                    print(f"Review score: {avg_score}")
                else:
                    logger.warning(f"Review returned None average_score for initial idea generation (subject: {selected_subject})")
                if review_data:
                    if "scores" in review_data:
                        first_idea_state.review_scores = review_data["scores"]
                    if "reviews" in review_data:
                        first_idea_state.review_feedback = review_data["reviews"]
                    if avg_score is not None:
                        first_idea_state.average_score = avg_score
                        first_idea_state.reward = avg_score
                    else:
                        first_idea_state.average_score = 0.0
                        first_idea_state.reward = 0.0
                
                # Add the first generated idea as a child of the root node
                first_idea_node = current_root.add_child(first_idea_state, "generate")
                
                # Set current node to the first idea node
                current_node = first_idea_node
                
                chat_messages.append({"role": "assistant", "content": llm_response})
                
            # Subsequent messages: Treat as direct feedback to improve the current idea
            else:
                # Add system message indicating feedback processing
                chat_messages.append(
                    {"role": "system", "content": "Processing your feedback to improve the research idea..."}
                )
                
                # Process user feedback using the ideation agent
                improved_content, raw_output = ideation_agent.process_feedback(
                    idea=main_idea,
                    user_feedback=user_message,
                    original_raw_output=getattr(current_node.state, "raw_llm_output", None),
                    subject=getattr(current_node.state, "subject", None) or selected_subject
                )
                
                # Get the current feedback dictionary and add the new message
                current_feedback = current_node.state.feedback.copy() if hasattr(current_node.state, "feedback") else {}
                # Add the new feedback with a timestamp as key
                from datetime import datetime
                feedback_key = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                current_feedback[feedback_key] = user_message
                
                # Create a new state with updated idea based on feedback
                subject_from_state = getattr(current_node.state, "subject", None) or selected_subject
                new_state = MCTSState(
                    research_goal=current_node.state.research_goal,
                    current_idea=improved_content,
                    retrieved_knowledge=current_node.state.retrieved_knowledge.copy(),
                    feedback=current_feedback,  # Pass feedback as dictionary
                    depth=current_node.state.depth + 1,
                    reward=0.0,  # Initial reward will be updated with review score
                    subject=subject_from_state
                )
                
                # Get review using the unified review method
                review_data = review_agent.unified_review(improved_content, subject=subject_from_state)
                avg_score = review_data.get("average_score") if review_data else None
                if avg_score is not None:
                    print(f"Review score: {avg_score}")
                else:
                    logger.warning(f"Review returned None average_score for feedback improvement (subject: {subject_from_state})")
                if review_data:
                    if "scores" in review_data:
                        new_state.review_scores = review_data["scores"]
                    if "reviews" in review_data:
                        new_state.review_feedback = review_data["reviews"]
                    if avg_score is not None:
                        new_state.average_score = avg_score
                        new_state.reward = avg_score
                    else:
                        new_state.average_score = 0.0
                        new_state.reward = 0.0
                
                # Create new node and add as child of current node
                new_node = current_node.add_child(new_state, "direct_feedback")
                current_node = new_node
                
                # Update main idea
                main_idea = improved_content
                
                # Add system message acknowledging feedback incorporation
                chat_messages.append(
                    {"role": "system", "content": "Research idea updated based on your feedback."}
                )
                
                # Add the improved idea as an assistant message
                chat_messages.append({"role": "assistant", "content": improved_content})
                
            # Return the updated state
            return jsonify(
                {
                    "messages": chat_messages,
                    "idea": main_idea,
                    "initial_proposal": current_root.state.research_goal if current_root else user_message,
                    "review_scores": getattr(current_node.state, "review_scores", {}),
                    "average_score": getattr(current_node.state, "average_score", 0.0),
                    "feedback": get_latest_feedback(getattr(current_node.state, "feedback", {})),
                }
            )
                
        except Exception as e:
            error_message = f"Error processing chat: {str(e)}"
            traceback.print_exc()  # Print the stack trace for debugging
            chat_messages.append({"role": "system", "content": error_message})
            return jsonify({"error": error_message}), 500

#This is the step function for the simple UCT algorithm
# @app.route("/api/step", methods=["POST"])
# def step():
#     global main_idea, current_node, current_state
#     if current_node is None:
#         return jsonify({"error": "Please enter an initial research idea first"}), 400

#     data = request.get_json()
#     if not data or "action" not in data:
#         return jsonify({"error": "Invalid payload"}), 400

#     action = data["action"]
#     use_mcts = data.get('use_mcts', False)

#     try:
#         # Handle the automatic generation action using UCT algorithm
#         if action == "generate":
#             # Automatically select an action using UCT algorithm
#             # First get valid actions
#             valid_actions = ["review_and_refine", "retrieve_and_refine", "refresh_idea"]
            
#             # If use_mcts is True, log that we're using the MCTS algorithm
#             if use_mcts:
#                 chat_messages.append({
#                     "role": "system", 
#                     "content": "Using MCTS algorithm for automated exploration..."
#                 })
#             # Simple UCT action selection
#             def uct_score(action):
#                 # Get the child node for this action if it exists
#                 child = None
#                 for c in current_node.children:
#                     if c.action == action:
#                         child = c
#                         break
                
#                 # If no child exists for this action, it's unexplored (high potential)
#                 if not child:
#                     return float('inf')
                
#                 # Calculate UCT score with exploration constant 1.414 (standard)
#                 exploitation = child.value  # Use existing value property 
#                 exploration = 1.414 * math.sqrt(math.log(current_node.visits) / max(child.visits, 1))
                
#                 # Add domain-specific heuristics
#                 bonus = 0
#                 # If low novelty score, prefer retrieve_and_refine
#                 if action == "retrieve_and_refine" and hasattr(current_node.state, "review_scores"):
#                     if current_node.state.review_scores.get("novelty", 10) < 7:
#                         bonus += 0.2
                
#                 # If iteration count is high, prefer refresh_idea occasionally
#                 if action == "refresh_idea" and current_node.depth > 3:
#                     bonus += 0.15
                
#                 return exploitation + exploration + bonus
            
#             # Select action with highest UCT score
#             selected_action = max(valid_actions, key=uct_score)
            
#             # Add system message about the selected action
#             chat_messages.append({
#                 "role": "system", 
#                 "content": f"Auto-generating using '{selected_action.replace('_', ' ')}' strategy..."
#             })
            
#             # Recursively call this function with the selected action
#             return step_action(selected_action)
            
#         # Add handler for the judge action - needed by review_and_refine
#         elif action == "judge":
#             # Use the review agent to get a unified review of the current idea
#             subject_from_state = getattr(current_node.state, "subject", None) or selected_subject
            review_data = review_agent.unified_review(current_node.state.current_idea, subject=subject_from_state)
            
#             # Add the review scores to the current node's state
#             if review_data:
#                 if not hasattr(current_node.state, "review_scores") or not current_node.state.review_scores:
#                     current_node.state.review_scores = {}
#                 if "scores" in review_data:
#                     current_node.state.review_scores = review_data["scores"]
#                 if "reviews" in review_data:
#                     current_node.state.review_feedback = review_data["reviews"]
#                 if "average_score" in review_data:
#                     current_node.state.average_score = review_data["average_score"]
            
#             # Add system message with review summary
#             avg_score = review_data.get("average_score", 0)
#             chat_messages.append({
#                 "role": "system", 
#                 "content": f"Review complete. Overall score: {avg_score:.1f}/10"
#             })
            
#             # Return the review data
#             return jsonify({
#                 "idea": main_idea,
#                 "nodeId": current_node.id,
#                 "action": action,
#                 "depth": current_node.state.depth,
#                 "review_scores": review_data.get("scores", {}),
#                 "average_score": review_data.get("average_score", 0.0),
#                 "review_feedback": review_data.get("reviews", {})
#             })
        
#         # Handle regular actions with their existing implementation
#         elif action == "review_and_refine":
#             # First get unified review
#             subject_from_state = getattr(current_node.state, "subject", None) or selected_subject
            review_data = review_agent.unified_review(current_node.state.current_idea, subject=subject_from_state)
            
#             # Sort aspects by score to find lowest scoring ones
#             aspect_scores = []
#             if "scores" in review_data:
#                 aspect_scores = sorted(
#                     review_data["scores"].items(),
#                     key=lambda x: x[1]
#                 )[:3]  # Get 3 lowest scoring aspects
            
#             # Get detailed reviews for lowest aspects
#             detailed_reviews = []
#             for aspect, score in aspect_scores:
#                 review = structured_review_agent.review_aspect(
#                     current_node.state.current_idea,
#                     aspect
#                 )
#                 if review:
#                     detailed_reviews.append(review)
            
#             # Create improvement prompt with focused feedback
#             improvement_state = MCTSState(
#                 research_goal=current_node.state.research_goal,
#                 current_idea=current_node.state.current_idea,
#                 retrieved_knowledge=current_node.state.retrieved_knowledge.copy(),
#                 feedback=detailed_reviews,
#                 depth=current_node.state.depth + 1
#             )
            
#             # Get improved idea from ideation agent
#             response = mcts.ideation_agent.execute_action(
#                 "review_and_refine",
#                 {
#                     "current_idea": current_node.state.current_idea,
#                     "reviews": detailed_reviews,
#                     "action_type": "execute"
#                 }
#             )
            
#             # Update state with improved idea
#             improvement_state.current_idea = response["content"]
            
#             # Get new review scores
#             new_review = review_agent.unified_review(improvement_state.current_idea)
#             if new_review:
#                 improvement_state.review_scores = new_review.get("scores", {})
#                 improvement_state.review_feedback = new_review.get("reviews", {})
#                 improvement_state.average_score = new_review.get("average_score", 0.0)
#                 improvement_state.reward = new_review.get("average_score", 0.0) / 10
            
#             # Create new node and update current
#             new_node = current_node.add_child(improvement_state, action)
#             current_node = new_node
#             main_idea = improvement_state.current_idea

#         elif action == "retrieve_and_refine":
#             # Generate query based on current idea
#             query_state = {
#                 "current_idea": current_node.state.current_idea,
#                 "action_type": "generate_query"
#             }
#             query_response = mcts.ideation_agent.execute_action("generate_query", query_state)
            
#             # Add system message about query generation
#             chat_messages.append({
#                 "role": "system",
#                 "content": "Generating search query..."
#             })
            
#             # Get search query
#             query = None
#             try:
#                 if isinstance(query_response.get("content"), str):
#                     import json
#                     # Try to extract query from JSON in content
#                     match = re.search(r'{.*}', query_response["content"])
#                     if match:
#                         query_json = json.loads(match.group())
#                         query = query_json.get("query")
#                     if not query:
#                         # Fallback to using first sentence
#                         query = query_response["content"].split(".")[0]

#                     # Add message showing the generated query
#                     chat_messages.append({
#                         "role": "system", 
#                         "content": f"Generated search query: {query}"
#                     })

#             except Exception as e:
#                 print(f"Error parsing query: {e}")
#                 query = current_node.state.current_idea[:100]  # Fallback to using truncated idea
#                 chat_messages.append({
#                     "role": "system",
#                     "content": f"Failed to parse query, using fallback: {query}"
#                 })
            
#             # Retrieve relevant papers
#             chat_messages.append({
#                 "role": "system",
#                 "content": "Searching for relevant papers..."
#             })
            
#             search_results = scholar_qa.answer_query(query)
            
#             # Add message showing search results
#             if search_results and "sections" in search_results:
#                 chat_messages.append({
#                     "role": "system",
#                     "content": f"Found {len(search_results['sections'])} relevant sections from papers"
#                 })

#             # Create new state with retrieved knowledge
#             retrieval_state = MCTSState(
#                 research_goal=current_node.state.research_goal,
#                 current_idea=current_node.state.current_idea,
#                 retrieved_knowledge=current_node.state.retrieved_knowledge + [search_results],
#                 feedback=current_node.state.feedback.copy(),
#                 depth=current_node.state.depth + 1
#             )
            
#             # Improve idea with retrieved knowledge
#             improvement_response = mcts.ideation_agent.execute_action(
#                 "retrieve_and_refine",
#                 {
#                     "current_idea": current_node.state.current_idea,
#                     "retrieved_content": search_results,
#                     "action_type": "execute"
#                 }
#             )
            
#             # Update state with improved idea
#             retrieval_state.current_idea = improvement_response["content"]
            
#             # Get new review scores
#             new_review = review_agent.unified_review(retrieval_state.current_idea)
#             if new_review:
#                 retrieval_state.review_scores = new_review.get("scores", {})
#                 retrieval_state.review_feedback = new_review.get("reviews", {})
#                 retrieval_state.average_score = new_review.get("average_score", 0.0)
#                 retrieval_state.reward = new_review.get("average_score", 0.0) / 10
            
#             # Create new node and update current
#             new_node = current_node.add_child(retrieval_state, action)
#             current_node = new_node
#             main_idea = retrieval_state.current_idea

#         elif action == "refresh_idea":
#             # Get the research goal from the root node
#             research_goal = None
#             if hasattr(current_root.state, "research_goal"):
#                 research_goal = current_root.state.research_goal

#             # Get fresh perspective on the idea
#             response = mcts.ideation_agent.execute_action(
#                 "refresh_idea",
#                 {
#                     "research_goal": research_goal,
#                     "current_idea": current_node.state.current_idea,
#                     "action_type": "execute"
#                 }
#             )
            
#             # Create a new state with trajectory-level memory
#             # Start with depth 1 since this is directly connected to the root
#             refresh_state = MCTSState(
#                 research_goal=research_goal,
#                 current_idea=response["content"],
#                 retrieved_knowledge=[],  # Start with empty retrieved knowledge for new approach
#                 feedback={},  # Start with empty feedback for new approach
#                 depth=1  # Directly connected to root, so depth is 1
#             )
            
#             # Get new review scores
#             new_review = review_agent.unified_review(refresh_state.current_idea)
#             if new_review:
#                 refresh_state.review_scores = new_review.get("scores", {})
#                 refresh_state.review_feedback = new_review.get("reviews", {})
#                 refresh_state.average_score = new_review.get("average_score", 0.0)
#                 refresh_state.reward = new_review.get("average_score", 0.0) / 10
            
#             # Create new node and add as child of the ROOT node instead of current node
#             new_node = current_root.add_child(refresh_state, action)
            
#             # Update current node to the newly created node
#             current_node = new_node
#             main_idea = refresh_state.current_idea

#             # Add system message about the refresh
#             chat_messages.append({
#                 "role": "system", 
#                 "content": "Created a new approach based on the original research goal."
#             })

#         else:
#             return jsonify({"error": "Invalid action"}), 400

#         # Return updated state
#         return jsonify({
#             "idea": main_idea,
#             "nodeId": current_node.id,
#             "action": action,
#             "depth": current_node.state.depth,
#             "review_scores": getattr(current_node.state, "review_scores", {}),
#             "average_score": getattr(current_node.state, "average_score", 0.0),
#             "retrieved_knowledge": bool(current_node.state.retrieved_knowledge),
#             "has_feedback": bool(current_node.state.feedback)
#         })

#     except Exception as e:
#         error_message = f"Error executing {action}: {str(e)}"
#         traceback.print_exc()
#         return jsonify({"error": error_message}), 500

@app.route("/api/step", methods=["POST"])
def step():
    global main_idea, current_node, current_state, exploration_in_progress, current_root
    
    if current_node is None:
        return jsonify({"error": "Please enter an initial research idea first"}), 400

    data = request.get_json()
    if not data or "action" not in data:
        return jsonify({"error": "Invalid payload"}), 400

    action = data["action"]
    use_mcts = data.get('use_mcts', False)
    num_iterations = data.get('num_iterations', 1)
    max_iterations = data.get('max_iterations', 5)

    try:
        # Enhanced MCTS implementation following Algorithm 1 from the PDF
        if action == "generate":
            # Prevent concurrent MCTS executions
            if exploration_in_progress:
                return jsonify({"error": "MCTS exploration already in progress"}), 429
            
            exploration_in_progress = True
            
            try:
                
                # Perform MCTS iterations
                best_node = current_node
                if use_mcts and num_iterations <= max_iterations:
                    
                    # Phase 1: SELECT - Traverse tree using UCT to find leaf node
                    selected_node = mcts_select(current_root)
                    
                    # Phase 2: EVALUATE - Get reward for the selected state
                    reward = mcts_evaluate(selected_node)
                    
                    # Phase 3: EXPAND - Create children if not terminal and below max depth
                    if selected_node.state.depth < mcts.config["experiment"]["max_depth"]:
                        mcts_expand(selected_node)
                    
                    # Phase 4: BACKPROPAGATE - Update Q and N values up the tree
                    mcts_backpropagate(selected_node, reward)
                    
                    # Track the best node found so far
                    if reward > (getattr(best_node.state, 'average_score', 0) / 10.0):
                        best_node = selected_node
                
                # Select the best child of root after all iterations
                final_best = mcts_best_child(current_root)
                if final_best and hasattr(final_best.state, 'average_score'):
                    if final_best.state.average_score > (getattr(best_node.state, 'average_score', 0)):
                        best_node = final_best
                
                # Update current state to the best found
                current_node = best_node
                main_idea = best_node.state.current_idea
                
                chat_messages.append({
                    "role": "system", 
                    "content": f"âœ… MCTS completed. Best score: {getattr(best_node.state, 'average_score', 0):.1f}/10"
                })
                
                return jsonify({
                    "idea": main_idea,
                    "nodeId": current_node.id,
                    "action": "mcts_exploration",
                    "depth": current_node.state.depth,
                    "visits": current_node.visits,
                    "value": current_node.value,
                    "review_scores": getattr(current_node.state, "review_scores", {}),
                    "average_score": getattr(current_node.state, "average_score", 0.0),
                    "messages": chat_messages[-5:]  # Return last 5 messages
                })
                
            finally:
                exploration_in_progress = False
        
        # Add handler for the judge action - needed by review_and_refine
        elif action == "judge":
            # Use the review agent to get a unified review of the current idea
            subject_from_state = getattr(current_node.state, "subject", None) or selected_subject
            review_data = review_agent.unified_review(current_node.state.current_idea, subject=subject_from_state)
            
            # Add the review scores to the current node's state
            if review_data:
                if not hasattr(current_node.state, "review_scores") or not current_node.state.review_scores:
                    current_node.state.review_scores = {}
                if "scores" in review_data:
                    current_node.state.review_scores = review_data["scores"]
                if "reviews" in review_data:
                    current_node.state.review_feedback = review_data["reviews"]
                avg_score = review_data.get("average_score")
                if avg_score is not None:
                    current_node.state.average_score = avg_score
                else:
                    logger.warning(f"Review returned None average_score for judge action (subject: {subject_from_state})")
                    current_node.state.average_score = 0.0
            
            # Add system message with review summary
            avg_score = review_data.get("average_score") if review_data else 0
            chat_messages.append({
                "role": "system", 
                "content": f"Review complete. Overall score: {avg_score:.1f}/10"
            })
            
            # Return the review data
            return jsonify({
                "idea": main_idea,
                "nodeId": current_node.id,
                "action": action,
                "depth": current_node.state.depth,
                "review_scores": review_data.get("scores", {}),
                "average_score": review_data.get("average_score", 0.0),
                "review_feedback": review_data.get("reviews", {})
            })
        
        # Handle regular actions with their existing implementation
        elif action == "review_and_refine":
            # First get unified review
            subject_from_state = getattr(current_node.state, "subject", None) or selected_subject
            review_data = review_agent.unified_review(current_node.state.current_idea, subject=subject_from_state)
            
            # Sort aspects by score to find lowest scoring ones
            aspect_scores = []
            if "scores" in review_data:
                aspect_scores = sorted(
                    review_data["scores"].items(),
                    key=lambda x: x[1]
                )[:3]  # Get 3 lowest scoring aspects
            
            # Get detailed reviews for lowest aspects
            detailed_reviews = []
            for aspect, score in aspect_scores:
                review = structured_review_agent.review_aspect(
                    current_node.state.current_idea,
                    aspect
                )
                if review:
                    detailed_reviews.append(review)
            
            # Create improvement prompt with focused feedback
            improvement_state = MCTSState(
                research_goal=current_node.state.research_goal,
                current_idea=current_node.state.current_idea,
                retrieved_knowledge=current_node.state.retrieved_knowledge.copy(),
                feedback=detailed_reviews,
                depth=current_node.state.depth + 1,
                subject=subject_from_state
            )
            
            # Get improved idea from ideation agent
            response = mcts.ideation_agent.execute_action(
                "review_and_refine",
                {
                    "current_idea": current_node.state.current_idea,
                    "reviews": detailed_reviews,
                    "action_type": "execute",
                    "subject": subject_from_state
                }
            )
            
            # Update state with improved idea
            improvement_state.current_idea = response["content"]
            
            # Get new review scores
            new_review = review_agent.unified_review(improvement_state.current_idea, subject=subject_from_state)
            if new_review:
                improvement_state.review_scores = new_review.get("scores", {})
                improvement_state.review_feedback = new_review.get("reviews", {})
                improvement_state.average_score = new_review.get("average_score", 0.0)
                improvement_state.reward = new_review.get("average_score", 0.0) / 10
            
            # Create new node and update current
            new_node = current_node.add_child(improvement_state, action)
            current_node = new_node
            main_idea = improvement_state.current_idea

        # IMPLEMENT MISSING retrieve_and_refine ACTION
        elif action == "retrieve_and_refine":
            # Step 1: Generate search query based on current idea
            chat_messages.append({
                "role": "system",
                "content": "Generating search query for knowledge retrieval..."
            })
            
            # Generate query using ideation agent (consistent with frontend)
            query_response = mcts.ideation_agent.execute_action(
                "generate_query",
                {
                    "current_idea": current_node.state.current_idea,
                    "action_type": "generate_query"
                }
            )
            
            # Extract query from response
            query = None
            try:
                content = query_response.get("content", "")
                # Try to parse JSON response first
                json_match = re.search(r'{.*}', content)
                if json_match:
                    query_json = json.loads(json_match.group())
                    query = query_json.get("query", content.split(".")[0])
                else:
                    # Fallback to using first sentence
                    query = content.split(".")[0] if content else current_node.state.current_idea[:100]
                    
                chat_messages.append({
                    "role": "system", 
                    "content": f"Generated search query: {query}"
                })
                
            except Exception as e:
                print(f"Error parsing query: {e}")
                query = current_node.state.current_idea[:100]  # Fallback
                
            # Step 2: Retrieve relevant knowledge using ScholarQA
            chat_messages.append({
                "role": "system",
                "content": "Searching for relevant papers..."
            })
            
            try:
                search_results = scholar_qa.answer_query(query)
                
                if search_results and "sections" in search_results:
                    chat_messages.append({
                        "role": "system",
                        "content": f"Found {len(search_results['sections'])} relevant sections from papers"
                    })
                else:
                    search_results = {"sections": [], "query": query}
                    chat_messages.append({
                        "role": "system",
                        "content": "No relevant papers found, proceeding without additional knowledge"
                    })
                    
            except Exception as e:
                print(f"Error in knowledge retrieval: {e}")
                search_results = {"sections": [], "query": query}
                chat_messages.append({
                    "role": "system",
                    "content": f"Error in retrieval: {str(e)}"
                })
            
            # Step 3: Improve idea with retrieved knowledge
            chat_messages.append({
                "role": "system",
                "content": "Refining idea with retrieved knowledge..."
            })
            
            # Create new state with retrieved knowledge
            retrieval_state = MCTSState(
                research_goal=current_node.state.research_goal,
                current_idea=current_node.state.current_idea,
                retrieved_knowledge=current_node.state.retrieved_knowledge + [search_results],
                feedback=current_node.state.feedback.copy(),
                depth=current_node.state.depth + 1
            )
            
            # Improve idea with retrieved knowledge using ideation agent
            improvement_response = mcts.ideation_agent.execute_action(
                "retrieve_and_refine",
                {
                    "current_idea": current_node.state.current_idea,
                    "retrieved_content": search_results,
                    "action_type": "execute"
                }
            )
            
            # Update state with improved idea
            retrieval_state.current_idea = improvement_response["content"]
            
            # Step 4: Get new review scores for the improved idea
            subject_from_state = getattr(current_node.state, "subject", None) or selected_subject
            new_review = review_agent.unified_review(retrieval_state.current_idea, subject=subject_from_state)
            if new_review:
                avg_score = new_review.get("average_score")
                if avg_score is not None:
                    retrieval_state.review_scores = new_review.get("scores", {})
                    retrieval_state.review_feedback = new_review.get("reviews", {})
                    retrieval_state.average_score = avg_score
                    retrieval_state.reward = avg_score / 10
                else:
                    logger.warning(f"Review returned None average_score for retrieve_and_refine action")
                    retrieval_state.average_score = 0.0
                    retrieval_state.reward = 0.0
            
            # Create new node and update current
            new_node = current_node.add_child(retrieval_state, action)
            current_node = new_node
            main_idea = retrieval_state.current_idea
            
            chat_messages.append({
                "role": "system",
                "content": f"Idea refined with retrieved knowledge. New score: {getattr(retrieval_state, 'average_score', 0):.1f}/10"
            })

        elif action == "refresh_idea":
            # Get the research goal from the root node
            research_goal = None
            if hasattr(current_root.state, "research_goal"):
                research_goal = current_root.state.research_goal

            # Get fresh perspective on the idea
            response = mcts.ideation_agent.execute_action(
                "refresh_idea",
                {
                    "research_goal": research_goal,
                    "current_idea": current_node.state.current_idea,
                    "action_type": "execute"
                }
            )
            
            # Create a new state with trajectory-level memory
            # # Start with depth 1 since this is directly connected to the root
            # parent_depth = current_node.parent.state.depth if current_node.parent else 0
            if response and "content" in response:
                refresh_state = MCTSState(
                    research_goal=research_goal,
                    current_idea=response["content"],
                    retrieved_knowledge=[],  # Start with empty retrieved knowledge for new approach
                    feedback={},  # Start with empty feedback for new approach
                    depth=current_node.state.depth +1  # Directly connected to root, so depth is 1
                )
            
            # Get new review scores
            subject_from_state = getattr(current_node.state, "subject", None) or selected_subject
            new_review = review_agent.unified_review(refresh_state.current_idea, subject=subject_from_state)
            if new_review:
                avg_score = new_review.get("average_score")
                if avg_score is not None:
                    refresh_state.review_scores = new_review.get("scores", {})
                    refresh_state.review_feedback = new_review.get("reviews", {})
                    refresh_state.average_score = avg_score
                    refresh_state.reward = avg_score / 10
                else:
                    logger.warning(f"Review returned None average_score for refresh_idea action")
                    refresh_state.average_score = 0.0
                    refresh_state.reward = 0.0
            
            # FIXED: Create new node as child of current node's PARENT (sibling relationship)
            if current_node.parent is not None:
                # Current node has a parent - create sibling
                parent_node = current_node.parent
                new_node = parent_node.add_child(refresh_state, action)
            else:
                # Current node IS the root - create child of root
                new_node = current_root.add_child(refresh_state, action)
            
            # Update current node to the newly created node
            current_node = new_node
            main_idea = refresh_state.current_idea

            # Add system message about the refresh
            chat_messages.append({
                "role": "system", 
                "content": "Created a new approach based on the original research goal."
            })

        elif action == "generate_ia_topic":
            # Handle Physics IA topic generation
            research_goal = data.get("research_goal", "")
            topics = data.get("topics", [])  # Get all topics from syllabus
            
            # Get subject from state or request
            subject_from_state = getattr(current_node.state, "subject", None) or selected_subject
            
            # Generate IA topic using ideation agent
            response = mcts.ideation_agent.execute_action(
                "generate_ia_topic",
                {
                    "research_goal": research_goal,
                    "current_state": current_node.state,
                    "subject": subject_from_state,
                    "topics": topics,  # Pass all topics from syllabus
                    "selected_topics": topics  # Also pass as selected_topics for prompt compatibility
                }
            )
            
            ia_topic = response.get("content", "").strip()
            
            # Update state with IA topic
            if not hasattr(current_node.state, "ia_topic"):
                current_node.state.ia_topic = ia_topic
            if topics:
                current_node.state.selected_topics = topics
            if not hasattr(current_node.state, "assessment_type"):
                current_node.state.assessment_type = "IA"
            
            # Update main idea
            main_idea = ia_topic
            
            chat_messages.append({
                "role": "system",
                "content": "IA topic generated successfully."
            })
            
            return jsonify({
                "idea": ia_topic,
                "nodeId": current_node.id,
                "action": action,
                "depth": current_node.state.depth
            })

        else:
            return jsonify({"error": "Invalid action"}), 400

        # Return updated state
        return jsonify({
            "idea": main_idea,
            "nodeId": current_node.id,
            "action": action,
            "depth": current_node.state.depth,
            "review_scores": getattr(current_node.state, "review_scores", {}),
            "average_score": getattr(current_node.state, "average_score", 0.0),
            "retrieved_knowledge": bool(current_node.state.retrieved_knowledge),
            "has_feedback": bool(current_node.state.feedback),
            "feedback": get_latest_feedback(getattr(current_node.state, "feedback", {}))
        })

    except Exception as e:
        exploration_in_progress = False
        error_message = f"Error executing {action}: {str(e)}"
        traceback.print_exc()
        return jsonify({"error": error_message}), 500


def execute_mcts_action(state, action):
    """Execute an action within MCTS to create a new state"""
    try:
        if action == "review_and_refine":
            # Get current reviews if not available
            if not hasattr(state, "review_scores") or not state.review_scores:
                # Get subject from state or global selected_subject
                subject_from_state = getattr(state, "subject", None) or selected_subject
                if not subject_from_state:
                    logger.warning(f"No subject found in state for review_and_refine action, review may use default prompts")
                
                review_data = review_agent.unified_review(state.current_idea, subject=subject_from_state)
                if review_data:
                    avg_score = review_data.get("average_score")
                    # Handle None average_score (indicates review failure)
                    if avg_score is None:
                        logger.warning(f"Review returned None average_score for review_and_refine action (subject: {subject_from_state}). This indicates a review failure.")
                        state.average_score = 0.0
                    else:
                        state.review_scores = review_data.get("scores", {})
                        state.review_feedback = review_data.get("reviews", {})
                        state.average_score = avg_score
                        logger.info(f"Review completed with score: {state.average_score} (subject: {subject_from_state})")
            
            # Find lowest scoring aspects for improvement
            if hasattr(state, "review_scores") and state.review_scores:
                sorted_aspects = sorted(state.review_scores.items(), key=lambda x: x[1])[:3]
                detailed_reviews = []
                for aspect, score in sorted_aspects:
                    if hasattr(state, "review_feedback") and aspect in state.review_feedback:
                        detailed_reviews.append({
                            "aspect": aspect,
                            "score": score,
                            "feedback": state.review_feedback[aspect]
                        })
            else:
                detailed_reviews = []
            
            # Get improved idea
            response = mcts.ideation_agent.execute_action(
                "review_and_refine",
                {
                    "current_idea": state.current_idea,
                    "reviews": detailed_reviews,
                    "action_type": "execute"
                }
            )
            
            # Create new state
            new_state = MCTSState(
                research_goal=state.research_goal,
                current_idea=response["content"],
                retrieved_knowledge=state.retrieved_knowledge.copy(),
                feedback=state.feedback.copy(),
                depth=state.depth + 1
            )
            
            return new_state
            
        elif action == "retrieve_and_refine":
            # FIXED: Implement proper retrieve_and_refine logic
            # Step 1: Generate search query
            query_response = mcts.ideation_agent.execute_action(
                "generate_query",
                {
                    "current_idea": state.current_idea,
                    "action_type": "generate_query"
                }
            )
            
            # Extract query from response
            query = None
            try:
                content = query_response.get("content", "")
                json_match = re.search(r'{.*}', content)
                if json_match:
                    query_json = json.loads(json_match.group())
                    query = query_json.get("query", content.split(".")[0])
                else:
                    query = content.split(".")[0] if content else state.current_idea[:100]
            except Exception as e:
                logger.error(f"Error parsing query: {e}")
                query = state.current_idea[:100]
            
            # Step 2: Retrieve knowledge
            try:
                search_results = scholar_qa.answer_query(query)
                if not search_results or "sections" not in search_results:
                    search_results = {"sections": [], "query": query}
            except Exception as e:
                logger.error(f"Error in knowledge retrieval: {e}")
                search_results = {"sections": [], "query": query}
            
            # Step 3: Improve idea with retrieved knowledge
            if search_results.get("sections"):
                improvement_response = mcts.ideation_agent.execute_action(
                    "retrieve_and_refine",
                    {
                        "current_idea": state.current_idea,
                        "retrieved_content": search_results,
                        "action_type": "execute"
                    }
                )
                
                new_state = MCTSState(
                    research_goal=state.research_goal,
                    current_idea=improvement_response["content"],
                    retrieved_knowledge=state.retrieved_knowledge + [search_results],
                    feedback=state.feedback.copy(),
                    depth=state.depth + 1
                )
                
                return new_state
            else:
                # No knowledge retrieved, return None to indicate failed action
                return None
            
        elif action == "refresh_idea":
            # Generate fresh approach
            response = mcts.ideation_agent.execute_action(
                "refresh_idea",
                {
                    "research_goal": state.research_goal,
                    "current_idea": state.current_idea,
                    "action_type": "execute"
                }
            )
            
            new_state = MCTSState(
                research_goal=state.research_goal,
                current_idea=response["content"],
                retrieved_knowledge=[],  # Fresh start
                feedback={},
                depth=1  # Connect to root level
            )
            
            return new_state
            
    except Exception as e:
        logger.error(f"Error executing MCTS action {action}: {e}")
        return None


def mcts_expand(node):
    """Phase 2: EXPAND - Add new child nodes for unexplored actions"""
    if node.state.depth >= mcts.config["experiment"]["max_depth"]:
        return
    
    valid_actions = ["review_and_refine", "retrieve_and_refine", "refresh_idea"]
    
    for action in valid_actions:
        if not any(child.action == action for child in node.children):
            try:
                # Execute action to create new state
                new_state = execute_mcts_action(node.state, action)
                
                if new_state:
                    # # SPECIAL HANDLING FOR REFRESH_IDEA - create sibling instead of child
                    # if action == "refresh_idea":
                    #     # Add as child of parent (sibling of current node)
                    #     parent_node = node.parent if node.parent else node  # Fallback to current if no parent
                    #     child = parent_node.add_child(new_state, action)
                    # else:
                    #     # Regular child creation for other actions
                    child = node.add_child(new_state, action)
                    child.state.depth = node.state.depth + 1
                    
                    # Evaluate the new child/sibling node
                    try:
                        # Get subject from node state or global selected_subject
                        subject_from_state = getattr(node.state, "subject", None) or getattr(new_state, "subject", None) or selected_subject
                        if not subject_from_state:
                            logger.warning(f"No subject found for node {node.id}, review may use default prompts")
                        
                        review_data = review_agent.unified_review(new_state.current_idea, subject=subject_from_state)
                        if review_data:
                            # Check if review_data has valid scores and average_score
                            scores = review_data.get("scores", {})
                            avg_score = review_data.get("average_score")
                            
                            # Only use review_data if it has valid scores or a valid average_score
                            # Handle None average_score (indicates review failure)
                            if avg_score is None:
                                logger.warning(f"Review returned None average_score for child node {child.id}. This indicates a review failure.")
                                child.state.average_score = 0.0
                                child.state.reward = 0.0
                            elif scores or (avg_score is not None and avg_score > 0):
                                child.state.review_scores = scores
                                child.state.review_feedback = review_data.get("reviews", {})
                                child.state.average_score = avg_score if avg_score is not None else 0.0
                                child.state.reward = child.state.average_score / 10
                                
                                logger.info(f"Child node {child.id} evaluated with score: {child.state.average_score} (subject: {subject_from_state})")
                            else:
                                logger.warning(f"Review data for child node {child.id} has no valid scores or average_score. Scores: {scores}, Average: {avg_score}")
                                # Don't set fallback score here - let it remain unset or use a more appropriate default
                                child.state.average_score = 0.0
                                child.state.reward = 0.0
                        else:
                            logger.warning(f"Review returned empty data for child node {child.id}")
                            child.state.average_score = 0.0
                            child.state.reward = 0.0
                            
                    except Exception as review_error:
                        logger.error(f"Error evaluating child node {child.id}: {review_error}", exc_info=True)
                        # Only set 5.0 as fallback if we're certain the review completely failed
                        # For now, use 0.0 to indicate no valid score rather than a misleading 5.0
                        child.state.average_score = 0.0
                        child.state.reward = 0.0
                    
                    logger.info(f"Expanded node {node.id} with action {action}, created {'sibling' if action == 'refresh_idea' else 'child'} {child.id}")
                
            except Exception as e:
                logger.error(f"Error expanding with action {action}: {e}")


def mcts_select(root_node):
    """Phase 1: SELECT - Traverse tree using UCT to find leaf node"""
    current = root_node
    path = [current]
    
    while True:
        # If node has unvisited children, select one randomly
        unvisited_children = [child for child in current.children if child.visits == 0]
        if unvisited_children:
            selected = random.choice(unvisited_children)
            path.append(selected)
            return selected
        
        # If all children visited, use UCT to select next node
        if not current.children:
            return current  # Leaf node
        
        # Calculate UCT scores for all children
        best_child = None
        best_uct = float('-inf')
        
        for child in current.children:
            if child.visits == 0:
                uct_value = float('inf')  # Unvisited nodes have infinite priority
            else:
                # UCT formula from PDF: Q(n)/N(n) + c * sqrt(ln(N(parent))/N(n))
                exploitation = child.value  # Q(n)/N(n) is already calculated in child.value
                exploration = math.sqrt(math.log(max(current.visits, 1)) / child.visits)
                uct_value = exploitation + (1.414 * exploration)  # c = sqrt(2) â‰ˆ 1.414
            
            if uct_value > best_uct:
                best_uct = uct_value
                best_child = child
        
        if best_child is None:
            return current
        
        current = best_child
        path.append(current)
        
        # If we've reached a leaf or terminal node, return it
        if not current.children or current.state.depth >= mcts.config["experiment"]["max_depth"]:
            return current


def mcts_evaluate(node):
    """Phase 2: EVALUATE - Get reward for the selected state via Review Agent"""
    try:
        # If node already has review scores, use them
        if hasattr(node.state, 'average_score') and node.state.average_score > 0:
            return node.state.average_score / 10.0  # Normalize to 0-1
        
        # Otherwise, get fresh review from Review Agent
        subject_from_state = getattr(node.state, "subject", None) or selected_subject
        review_data = review_agent.unified_review(node.state.current_idea, subject=subject_from_state)
        
        if review_data:
            avg_score = review_data.get("average_score")
            if avg_score is not None and avg_score > 0:
                # Update node state with review data
                node.state.review_scores = review_data.get("scores", {})
                node.state.review_feedback = review_data.get("reviews", {})
                node.state.average_score = avg_score
                node.state.reward = avg_score / 10.0
                
                return node.state.reward
            else:
                logger.warning(f"Review returned invalid average_score for node {node.id}: {avg_score}")
                return 0.0
        
        # Fallback reward based on depth (deeper = less reward)
        return 1.0 / (node.state.depth + 1)
        
    except Exception as e:
        logger.error(f"Error in MCTS evaluate: {e}")
        return 0.5  # Default middle reward



def mcts_backpropagate(node, reward):
    """Phase 4: BACKPROPAGATE - Update Q and N values for node and ancestors"""
    current = node
    current_reward = reward
    
    while current is not None:
        # Update visit count
        current.visits += 1
        
        # Update value using incremental average: value += (reward - value) / visits
        current.value += (current_reward - current.value) / current.visits
        
        # Apply discount factor for parent nodes
        current_reward *= mcts.discount_factor
        
        # Move to parent
        current = current.parent


def mcts_best_child(node):
    """Select the child with highest average reward Q/N"""
    if not node.children:
        return None
    
    best_child = None
    best_value = float('-inf')
    
    for child in node.children:
        if child.visits > 0 and child.value > best_value:
            best_value = child.value
            best_child = child
    
    return best_child


def best_node_by_score(root, min_safety=6.0, min_analysis=6.0):
    """Return the best reviewed node in the entire subtree by average_score.
    
    This is used for final "Best Idea" selection after MCTS automation completes.
    Separate from mcts_best_child() which uses MCTS value for exploration.
    
    Args:
        root: Root node of MCTS tree
        min_safety: Minimum safety_practicality score (default 6.0)
        min_analysis: Minimum data_analysis_viability score (default 6.0)
    
    Returns:
        Best node with highest average_score, or None if no viable node found
    """
    if root is None:
        return None
    
    best = None
    best_score = float("-inf")
    
    # Use iterative DFS to traverse entire tree
    stack = [root]
    while stack:
        node = stack.pop()
        # Add children to stack for traversal
        stack.extend(getattr(node, "children", []) or [])
        
        # Skip nodes without average_score (unreviewed)
        avg = getattr(node.state, "average_score", None)
        if avg is None or avg == 0:
            continue
        
        # Apply viability gates (check review_scores if available)
        scores = getattr(node.state, "review_scores", {}) or {}
        safety = scores.get("safety_practicality", 10)  # Default high if not present
        analysis = scores.get("data_analysis_viability", 10)  # Default high if not present
        
        # Skip nodes that don't meet viability thresholds
        if safety < min_safety or analysis < min_analysis:
            logger.info(f"Skipping node {node.id}: safety={safety}, analysis={analysis}")
            continue
        
        # Track best node
        if avg > best_score:
            best_score = avg
            best = node
    
    if best:
        logger.info(f"Best node selected: {best.id} with average_score={best_score}")
    else:
        logger.warning("No viable reviewed node found in tree")
    
    return best


# Helper function to avoid code duplication
def step_action(action):
    """Execute a step with a specific action and return the result"""
    # We simulate a new request with the selected action
    request_data = {"action": action}
    
    # Create a mock Request object
    class MockRequest:
        def get_json(self):
            return request_data
    
    # Store the original request
    original_request = request
    
    # Set the global request to our mock
    globals()["request"] = MockRequest()
    
    try:
        # Call the step function with our mock request
        result = step()
        return result
    finally:
        # Restore the original request
        globals()["request"] = original_request

@app.route("/api/tree", methods=["GET"])
def get_tree():
    if current_root is None:
        return jsonify({}), 200  # Return empty object instead of error

    def node_to_dict(node):
        # Get basic info about the node
        is_root = node.parent is None
        
        # For root node, use special formatting to show it's the research goal
        if is_root:
            node_data = {
                "id": node.id,
                "action": "research_goal",  # Special action type for root
                "idea": "RESEARCH GOAL: " + (node.state.research_goal[:80] + "..." if len(node.state.research_goal) > 80 else node.state.research_goal),
                "depth": node.state.depth,
                "reward": node.state.reward,
                "value": node.value,
                "visits": node.visits,
                "isCurrentNode": node.id == current_node.id if current_node else False,
                "state": {
                    "current_idea": node.state.research_goal,  # For root, use research_goal as current_idea
                    "depth": node.state.depth,
                    "reward": node.state.reward,
                    "hasReviews": False,  # Root node has no reviews
                    "hasRetrieval": False,  # Root node has no retrieval
                    "hasFeedback": False,  # Root node has no feedback
                    "isResearchGoal": True  # Flag to identify it's the research goal
                },
                "children": [node_to_dict(child) for child in node.children]
            }
        else:
            # Regular node formatting for ideas
            node_data = {
                "id": node.id,
                "action": node.action or "unknown",
                "idea": node.state.current_idea[:100] + "..." if len(node.state.current_idea) > 100 else node.state.current_idea,
                "depth": node.state.depth,
                "reward": node.state.reward,
                "value": node.value,
                "visits": node.visits,
                "isCurrentNode": node.id == current_node.id if current_node else False,
                "state": {
                    "current_idea": node.state.current_idea,
                    "depth": node.state.depth,
                    "reward": node.state.reward,
                    "hasReviews": hasattr(node.state, "review_scores") and bool(node.state.review_scores),
                    "hasRetrieval": bool(node.state.retrieved_knowledge),
                    "hasFeedback": bool(node.state.feedback),
                    "isResearchGoal": False  # Regular nodes are not research goals
                },
                "children": [node_to_dict(child) for child in node.children]
            }
            
            # Add review data if available
            if hasattr(node.state, "review_scores") and node.state.review_scores:
                node_data["reviews"] = {
                    "scores": node.state.review_scores,
                    "summary": getattr(node.state, "review_summary", {})
                }
        
        return node_data

    tree_data = node_to_dict(current_root)
    return jsonify(tree_data)


@app.route("/api/get_best_child", methods=["POST"])
def get_best_child():
    """Get the best node from the MCTS tree by average_score and navigate to it.
    
    This searches the entire tree (not just immediate children) for the node
    with the highest average_score that meets viability thresholds.
    """
    global current_root, current_node, main_idea
    
    try:
        if current_root is None:
            return jsonify({"error": "No MCTS tree available"}), 400
        
        # Use best_node_by_score to search entire tree by average_score
        best = best_node_by_score(current_root)
        
        if best is None:
            # No viable reviewed node found, fall back to current node
            if current_node is None:
                return jsonify({"error": "No reviewed viable idea found yet"}), 404
            
            logger.warning("No viable best node found, returning current node")
            return jsonify({
                "idea": current_node.state.current_idea,
                "average_score": getattr(current_node.state, "average_score", 0.0),
                "review_scores": getattr(current_node.state, "review_scores", {}),
                "nodeId": current_node.id,
                "depth": current_node.state.depth,
                "feedback": get_latest_feedback(getattr(current_node.state, "feedback", {}))
            })
        
        # Navigate to the best node (update current_node and main_idea)
        current_node = best
        main_idea = best.state.current_idea
        
        logger.info(f"Navigated to best node: {best.id} with score {getattr(best.state, 'average_score', 0.0)}")
        
        # Return best node's data
        return jsonify({
            "idea": best.state.current_idea,
            "average_score": getattr(best.state, "average_score", 0.0),
            "review_scores": getattr(best.state, "review_scores", {}),
            "feedback": get_latest_feedback(getattr(best.state, "feedback", {})),
            "nodeId": best.id,
            "depth": best.state.depth
        })
    
    except Exception as e:
        error_message = f"Error getting best node: {str(e)}"
        print(error_message)
        traceback.print_exc()
        return jsonify({"error": error_message}), 500

@app.route("/api/node", methods=["POST"])
def select_node():
    global current_node, main_idea, chat_messages
    data = request.get_json()
    if not data or "node_id" not in data:
        return jsonify({"error": "Invalid payload"}), 400

    node_id = data["node_id"]
    
    def find_node(root, target_id):
        """Recursively find a node by ID in the tree."""
        if root.id == target_id:
            return root
        for child in root.children:
            result = find_node(child, target_id)
            if result:
                return result
        return None

    # Find the node in the tree
    node = find_node(current_root, node_id)
    
    if node:
        # Update current node and idea
        current_node = node
        # Convert to string if it's not already
        idea_content = node.state.current_idea
        if isinstance(idea_content, (dict, list)):
            idea_content = json.dumps(idea_content)
        elif isinstance(idea_content, tuple):
            idea_content = str(idea_content)
        elif not isinstance(idea_content, str):
            idea_content = str(idea_content) if idea_content is not None else ""
        
        main_idea = idea_content
        
        # Log this action in chat
        chat_messages.append({
            "role": "system", 
            "content": f"Navigated to node {node_id} with action '{node.action or 'root'}'."
        })
        
        # Prepare response data
        response = {
            "idea": idea_content,
            "node_data": {
                "id": node.id,
                "action": node.action or "root",
                "depth": node.state.depth,
                "reward": node.state.reward
            },
            "review_scores": getattr(node.state, "review_scores", {}),
            "average_score": getattr(node.state, "average_score", 0.0),
            "feedback": get_latest_feedback(getattr(node.state, "feedback", {})),
        }
        
        # Include review data if available
        if hasattr(node.state, "review_scores") and node.state.review_scores:
            response["reviews"] = {
                "scores": node.state.review_scores,
                "feedback": node.state.review_feedback,
                "summary": getattr(node.state, "review_summary", {})
            }
        
        # Include trajectory history
        if node.parent:
            trajectory = []
            current = node
            while current.parent:
                trajectory.append({
                    "id": current.id,
                    "action": current.action,
                    "depth": current.state.depth
                })
                current = current.parent
            response["trajectory"] = list(reversed(trajectory))
        
        return jsonify(response)
    else:
        return jsonify({"error": f"Node with ID {node_id} not found"}), 404


@app.route("/api/idea", methods=["GET"])
def get_idea():
    if current_node is None:
        return jsonify({"idea": main_idea})
    
    return jsonify({
        "idea": main_idea,
        "review_scores": getattr(current_node.state, "review_scores", {}),
        "average_score": getattr(current_node.state, "average_score", 0.0),
        "feedback": get_latest_feedback(getattr(current_node.state, "feedback", {})),
    })


@app.route("/api/upload", methods=["POST"])
def upload_file():
    if "file" not in request.files:
        return jsonify({"error": "No file part"}), 400

    file = request.files["file"]
    if file.filename == "":
        return jsonify({"error": "No selected file"}), 400

    # Initialize abstract variable
    abstract = "Abstract could not be extracted"  # Default value in case no abstract is found

    if file and allowed_file(file.filename):
        filename = secure_filename(file.filename)
        unique_filename = f"{uuid.uuid4()}_{filename}"
        file_path = os.path.join(app.config["UPLOAD_FOLDER"], unique_filename)

        try:
            file.save(file_path)
            file_content = "File content could not be extracted"
            
            # Extract text content from PDF using PyMuPDF if it's a PDF
            if file_path.lower().endswith('.pdf'):
                try:
                    pdf_text = []
                    with pymupdf.open(file_path) as doc:
                        for page_num in range(len(doc)):
                            page = doc.load_page(page_num)
                            pdf_text.append(page.get_text())
                    file_content = "\n".join(pdf_text)
                    # Extract the abstract from the PDF content
                    abstract = extract_abstract(pdf_text)
                except Exception as pdf_err:
                    print(f"Error extracting PDF content: {pdf_err}")
            
            # Add file info to knowledge chunks
            new_id = len(knowledge_chunks) + 1
            chunk = {
                "id": new_id,
                "text": f"Uploaded file: {filename}",
                "full_text": file_content,
                "abstract": abstract,  # Added abstract here
                "source": file_path,
                "file_type": "attachment",
            }
            knowledge_chunks.append(chunk)

            return (
                jsonify(
                    {
                        "message": "File uploaded successfully",
                        "filename": filename,
                        "abstract": abstract,
                        "chunk": chunk,
                    }
                ),
                201,
            )

        except Exception as e:
            return jsonify({"error": f"Error saving file: {str(e)}"}), 500

    return jsonify({"error": "File type not allowed"}), 400


@app.route("/api/review_aspect", methods=["POST"])
def review_aspect():
    data = request.get_json()
    if not data or "idea" not in data or "aspect" not in data:
        return jsonify({"error": "Invalid payload"}), 400

    idea = data["idea"]
    aspect = data["aspect"]
    aspect_index = data.get("aspect_index", 0)

    try:
        print(f"\n=== REQUEST DATA ===")
        print(f"Aspect: {aspect}")
        print(f"Aspect Index: {aspect_index}")
        print(f"Idea length: {len(idea)} chars")
        
        review_data = structured_review_agent.review_aspect(idea, aspect)
        
        print(f"\n=== RESPONSE DATA ===")
        print(f"Review Data Keys: {review_data.keys()}")
        if 'highlight' in review_data:
            print(f"Highlight Data Keys: {review_data['highlight'].keys()}")
            print(f"Highlight Text: '{review_data['highlight'].get('text')}'")
            
            # Add debugging to check if highlight text exists in idea
            highlight_text = review_data['highlight'].get('text')
            if highlight_text:
                # Check for exact match
                exact_match = highlight_text in idea
                print(f"Exact match found in idea: {exact_match}")
                
                # Check for case-insensitive match
                case_insensitive_match = highlight_text.lower() in idea.lower()
                print(f"Case-insensitive match found in idea: {case_insensitive_match}")
                
                # Check if text might be affected by markdown formatting
                if not exact_match:
                    # Try to find position where highlight might be with markdown formatting
                    stripped_highlight = highlight_text.replace('*', '').replace('_', '')
                    stripped_match = stripped_highlight in idea
                    print(f"Match after removing markdown chars: {stripped_match}")
                    
                    # Additional check: Look for parts of the highlight text
                    if len(highlight_text) > 15:
                        partial_text = highlight_text[5:15]
                        partial_match = partial_text in idea
                        print(f"Partial match with '{partial_text}': {partial_match}")
        
        response = {"review_data": review_data, "aspect_index": aspect_index}
        print(f"\n=== SENDING RESPONSE ===")
        print(f"Response Keys: {response.keys()}")
        
        return jsonify(response)
    except Exception as e:
        error_message = f"Error reviewing {aspect}: {str(e)}"
        print(f"\n=== ERROR ===")
        print(error_message)
        return jsonify({"error": error_message}), 500


@app.route("/api/improve_idea", methods=["POST"])
def improve_idea():
    global main_idea, current_node
    data = request.get_json()
    if not data or "idea" not in data or "accepted_reviews" not in data:
        return jsonify({"error": "Invalid payload"}), 400

    idea = data["idea"]
    accepted_reviews = data["accepted_reviews"]

    print("Accepted reviews:", accepted_reviews)
    # print("Idea:", idea)

    try:
        # Use ideation_agent instead of structured_review_agent
        subject_from_state = getattr(current_node.state, "subject", None) if current_node else selected_subject
        improved_idea, raw_output = ideation_agent.improve_idea(idea, accepted_reviews, subject=subject_from_state)

        # Update the main idea in our application state
        main_idea = improved_idea

        # Create a new state with trajectory-level memory
        new_state = MCTSState(
            research_goal=current_node.state.research_goal,
            current_idea=improved_idea,
            retrieved_knowledge=current_node.state.retrieved_knowledge.copy(),
            feedback=current_node.state.feedback.copy(),
            depth=current_node.state.depth + 1,
            subject=subject_from_state
        )
        
        # Get review using the unified review method
        review_data = review_agent.unified_review(improved_idea, subject=subject_from_state)
        avg_score = review_data.get("average_score") if review_data else None
        if avg_score is not None:
            print(f"Review score: {avg_score}")
        else:
            logger.warning(f"Review returned None average_score for improve_idea action (subject: {subject_from_state})")
        if review_data:
            if "scores" in review_data:
                new_state.review_scores = review_data["scores"]
            if "reviews" in review_data:
                new_state.review_feedback = review_data["reviews"]
            if avg_score is not None:
                new_state.average_score = avg_score
                new_state.reward = avg_score
            else:
                new_state.average_score = 0.0
                new_state.reward = 0.0
        
        # Create new node and add as child of current node
        new_node = current_node.add_child(new_state, "review_and_refine")
        current_node = new_node

        # Add a system message about the improvement
        chat_messages.append(
            {
                "role": "system",
                "content": "Idea improved based on accepted review suggestions.",
            }
        )

        # Add the improved idea as an assistant message
        chat_messages.append({"role": "assistant", "content": improved_idea})

        return jsonify({
            "improved_idea": improved_idea,
            "review_scores": getattr(current_node.state, "review_scores", {}),
            "average_score": getattr(current_node.state, "average_score", 0.0),
            "feedback": get_latest_feedback(getattr(current_node.state, "feedback", {})),
        })
        
    except Exception as e:
        error_message = f"Error improving idea: {str(e)}"
        return jsonify({"error": error_message}), 500


@app.route("/api/generate_query", methods=["POST"])
def generate_query():
    """Generate a research query based on the current idea"""
    data = request.get_json()
    
    if not data or "idea" not in data:
        return jsonify({"error": "Missing idea in request"}), 400
    
    idea = data["idea"]
    
    try:
        # Log the attempt in chat
        global chat_messages
        chat_messages.append({
            "role": "system",
            "content": "Generating search query based on your research idea..."
        })
        
        # Use the ideation agent to generate a query
        prompt = "Given the following research idea, generate a concise and specific query for retrieving relevant scientific papers. The query should be focused on the key aspects of the research idea. Format your response as a JSON object with a single field named 'query'."
        
        # Call the ideation agent
        response = ideation_agent.execute_action(
            "generate_query",
            {"current_idea": idea, "prompt": prompt}
        )
        
        # Extract the query from the response
        content = response.get("content", "")
        
        # Try to parse JSON response
        try:
            # Look for JSON in the response
            json_match = re.search(r'```json\s*(.*?)\s*```|{.*}', content, re.DOTALL)
            if json_match:
                json_str = json_match.group(1) or json_match.group(0)
                query_data = json.loads(json_str)
                query = query_data.get("query", "")
            else:
                # Try to extract query directly
                query_match = re.search(r'query["\']?\s*:\s*["\']([^"\']+)["\']', content)
                if query_match:
                    query = query_match.group(1)
                else:
                    # Fallback - use the first sentence as query
                    query = content.split(".")[0]
        except Exception as e:
            # If JSON parsing fails, use a simple text extraction approach
            lines = content.split("\n")
            for line in lines:
                if "query" in line.lower():
                    query = line.split(":", 1)[1].strip().strip('"\'')
                    break
            else:
                query = content.strip()
        
        # Add the generated query to chat
        chat_messages.append({
            "role": "assistant",
            "content": f"**Generated search query:** \"{query}\"\n\nI'll use this query to find relevant papers. You can click 'Retrieve Knowledge' to proceed with this query."
        })
        
        return jsonify({"query": query})
    
    except Exception as e:
        error_trace = traceback.format_exc()
        print(f"Query generation error: {str(e)}\n{error_trace}")
        chat_messages.append({
            "role": "system",
            "content": f"Error generating query: {str(e)}"
        })
        return jsonify({"error": f"Failed to generate query: {str(e)}"}), 500

@app.route("/api/retrieve_knowledge", methods=["POST"])
def retrieve_knowledge():
    """Retrieve knowledge based on a query"""
    data = request.get_json()
    
    if not data or "query" not in data:
        return jsonify({"error": "Missing query in request"}), 400
    
    query = data["query"]
    
    try:
        # Log the attempt in chat
        global chat_messages
        chat_messages.append({
            "role": "system",
            "content": f"Searching for relevant papers using query: \"{query}\"..."
        })

        print(f"Retrieving knowledge for query: {query}")
        
        # Use ScholarQA to retrieve knowledge
        result = scholar_qa.answer_query(query)
        
        # Debug: Print result structure
        print(f"Result type: {type(result)}")
        print(f"Result keys: {result.keys() if isinstance(result, dict) else 'Not a dict'}")
        
        # Store the retrieval results globally
        global retrieval_results
        retrieval_results = result
        
        # Parse the sections and format for display
        formatted_sections = []
        
        # Handle both dict and Pydantic model results
        sections = result.get("sections", []) if isinstance(result, dict) else getattr(result, "sections", [])
        
        for section in sections:
            # Handle both dict and Pydantic model sections
            if isinstance(section, dict):
                section_dict = section
            else:
                section_dict = section.model_dump() if hasattr(section, 'model_dump') else section.dict() if hasattr(section, 'dict') else {}
            
            formatted_section = {
                "title": section_dict.get("title", "Untitled Section"),
                "summary": section_dict.get("tldr", "No summary available"),
                "content": section_dict.get("text", ""),
                "citations": []
            }
            
            # Extract citations
            citations = section_dict.get("citations", [])
        for citation in citations:
                # Handle both dict and Pydantic model citations
                if isinstance(citation, dict):
                    citation_dict = citation
                else:
                    citation_dict = citation.model_dump() if hasattr(citation, 'model_dump') else citation.dict() if hasattr(citation, 'dict') else {}
                
                citation_id = citation_dict.get("id", "")
                paper = citation_dict.get("paper", {})
                
                # Handle paper as dict or Pydantic model
                if not isinstance(paper, dict):
                    paper = paper.model_dump() if hasattr(paper, 'model_dump') else paper.dict() if hasattr(paper, 'dict') else {}
                
                # Safely extract authors - handle both dict and Pydantic model formats
                authors_list = paper.get("authors", [])
                if authors_list:
                    if isinstance(authors_list[0], dict):
                        authors = [author.get("name", "") for author in authors_list if isinstance(author, dict)]
                    else:
                        # Handle Pydantic model authors
                        authors = [author.name if hasattr(author, 'name') else str(author) for author in authors_list]
                else:
                    authors = []
                
                paper_url = extract_paper_url(citation_dict, paper)
                formatted_citation = {
                    "id": citation_id,
                    "title": paper.get("title", "Unknown paper"),
                    "authors": authors,
                    "year": paper.get("year", ""),
                    "venue": paper.get("venue", ""),
                    "url": paper_url
                }
                
                formatted_section["citations"].append(formatted_citation)
            
        formatted_sections.append(formatted_section)
        
        # Add success message to chat
        sections_count = len(formatted_sections)
        citations_count = sum(len(section["citations"]) for section in formatted_sections)
        
        note = ""
        if citations_count < 5:
            note = "\n\nNote: Fewer than 5 papers were found. Try a broader or alternative query for more results."
        chat_messages.append({
            "role": "assistant",
            "content": f"âœ… **Retrieval complete!** Found {sections_count} content sections with {citations_count} paper citations.\n\nPlease check the left panel to see the retrieved information.{note}"
        })
        
        return jsonify({
            "query": query,
            "sections": formatted_sections
        })
    
    except Exception as e:
        error_trace = traceback.format_exc()
        error_message = str(e)
        
        # Handle "no results" cases gracefully - return 200 with empty results instead of 500
        if "No relevant quotes extracted" in error_message or "No relevant papers found" in error_message:
            logger.info(f"No results found for query: {query}")
            chat_messages.append({
                "role": "system",
                "content": f"No relevant literature found for query: \"{query}\". Try refining your search terms or using a broader query."
            })
            return jsonify({
                "query": query,
                "sections": [],
                "message": "No relevant literature found. Please try a different or broader search query."
            }), 200
        
        # Check if it's a Semantic Scholar API 403 error
        if "403" in error_message or "S2 API" in error_message:
            error_message = "Semantic Scholar API authentication failed. Please check your SEMANTIC_SCHOLAR_API_KEY in the .env file. The API key may be invalid, expired, or missing required permissions."
        
        print(f"Retrieval error: {error_message}\n{error_trace}")
        chat_messages.append({
            "role": "system",
            "content": f"Error retrieving knowledge: {error_message}"
        })
        return jsonify({
            "error": error_message,
            "details": "If this is a 403 error, please verify your Semantic Scholar API key is valid and has the correct permissions."
        }), 500

@app.route("/api/improve_idea_with_knowledge", methods=["POST"])
def improve_idea_with_knowledge():
    """Improve the research idea based on retrieved knowledge."""
    global main_idea, current_node, retrieval_results
    
    data = request.get_json()
    if not data or "idea" not in data:
        return jsonify({"error": "Missing idea in request"}), 400
    
    idea = data["idea"]
    
    # Check if we have any retrieval results to use
    if not retrieval_results or "sections" not in retrieval_results:
        return jsonify({"error": "No retrieved knowledge available"}), 400
    
    try:
        # Format the retrieved knowledge for the ideation agent
        retrieved_content = []
        for section in retrieval_results.get("sections", []):
            section_text = f"## {section.get('title', 'Untitled')}\n\n"
            section_text += f"{section.get('text', '')}\n\n"
            
            # Add citations if available
            if section.get("citations"):
                section_text += "### References:\n"
                for citation in section.get("citations", []):
                    paper = citation.get("paper", {})
                    authors = ", ".join([author.get("name", "") for author in paper.get("authors", [])[:3]])
                    if len(paper.get("authors", [])) > 3:
                        authors += " et al."
                    section_text += f"- {paper.get('title', 'Untitled')} ({authors}, {paper.get('year', 'n.d.')})\n"
            
            retrieved_content.append(section_text)
        
        # Join all sections into a single text
        formatted_knowledge = "\n\n".join(retrieved_content)
        
        # Add system message about the improvement process
        chat_messages.append({
            "role": "system",
            "content": "Improving research idea with retrieved knowledge..."
        })
        
        # Add instructions to ensure markdown format and making only relevant improvements
        prompt_instructions = {
            "current_idea": idea,
            "retrieved_content": formatted_knowledge,
        }
        
        # Call the ideation agent to improve the idea based on the retrieved knowledge
        response = ideation_agent.execute_action(
            "refine_with_retrieval",
            prompt_instructions
        )
        
        # Extract the improved idea, handling potential JSON format
        content = response.get("content", "")
        improved_idea = content
        
        # Try to extract content from JSON if present
        try:
            # Look for JSON in response
            import re
            import json
            
            # Check if the response contains JSON
            json_match = re.search(r'```json\s*(.*?)\s*```|{.*}', content, re.DOTALL)
            if json_match:
                json_str = json_match.group(1) or json_match.group(0)
                # Clean up the extracted JSON string
                json_str = json_str.replace('\\n', ' ').strip()
                
                # Parse the JSON
                parsed_json = json.loads(json_str)
                if "content" in parsed_json:
                    improved_idea = parsed_json["content"]
                elif "text" in parsed_json:
                    improved_idea = parsed_json["text"]
                # If no content/text key but other text field exists, use that
                else:
                    for key in ["body", "description", "idea"]:
                        if key in parsed_json:
                            improved_idea = parsed_json[key]
                            break
        except Exception as json_error:
            print(f"JSON extraction error (continuing with raw content): {str(json_error)}")
            # If JSON parsing fails, keep the original content
            pass
        
        # Ensure improved_idea isn't empty
        if not improved_idea or improved_idea.strip() == "":
            improved_idea = content
        
        print(f"Improved idea length: {len(improved_idea)} characters")
        
        # Update the main idea in our application state
        main_idea = improved_idea
        
        # Create a new state with trajectory-level memory
        subject_from_state = getattr(current_node.state, "subject", None) or selected_subject
        new_state = MCTSState(
            research_goal=current_node.state.research_goal,
            current_idea=improved_idea,
            retrieved_knowledge=current_node.state.retrieved_knowledge.copy() + [retrieval_results.get("query", "")],
            feedback=current_node.state.feedback.copy(),
            depth=current_node.state.depth + 1,
            subject=subject_from_state
        )
        
        # Get review using the unified review method
        review_data = review_agent.unified_review(improved_idea, subject=subject_from_state)
        avg_score = review_data.get("average_score") if review_data else None
        if avg_score is not None:
            print(f"Review score: {avg_score}")
        else:
            logger.warning(f"Review returned None average_score for retrieve_and_refine route (subject: {subject_from_state})")
        if review_data:
            if "scores" in review_data:
                new_state.review_scores = review_data["scores"]
            if "reviews" in review_data:
                new_state.review_feedback = review_data["reviews"]
            if avg_score is not None:
                new_state.average_score = avg_score
                new_state.reward = avg_score
            else:
                new_state.average_score = 0.0
                new_state.reward = 0.0
        
        # Create new node and add as child of current node
        new_node = current_node.add_child(new_state, "retrieve_and_refine")
        current_node = new_node
        
        # Add a system message about the improvement
        chat_messages.append({
            "role": "system",
            "content": "Idea improved based on retrieved knowledge."
        })
        
        # Add the improved idea as an assistant message
        chat_messages.append({"role": "assistant", "content": improved_idea})
        
        return jsonify({
            "improved_idea": improved_idea,
            "content_length": len(improved_idea),
            "review_scores": getattr(new_state, "review_scores", {}),
            "average_score": getattr(new_state, "average_score", 0.0),
            "feedback": get_latest_feedback(getattr(new_state, "feedback", {})),
        })
        
    except Exception as e:
        error_message = f"Error improving idea with knowledge: {str(e)}"
        error_trace = traceback.format_exc()
        print(f"{error_message}\n{error_trace}")
        return jsonify({"error": error_message}), 500

@app.route("/api/refresh_idea", methods=["POST"])
def refresh_idea():
    """Dedicated endpoint for refreshing research ideas"""
    global main_idea, current_node, chat_messages, current_root
    
    # Check if current_node exists
    if current_node is None:
        return jsonify({"error": "No active research idea found. Please start by entering a research topic."}), 400
    
    try:
        # Add system message before refresh
        chat_messages.append(
            {"role": "system", "content": "Generating a completely new approach to the research goal..."}
        )
        
        # Get the research goal from the root node
        research_goal = None
        if hasattr(current_root.state, "research_goal"):
            research_goal = current_root.state.research_goal
        
        # Call the ideation agent to get a refreshed idea
        response = mcts.ideation_agent.execute_action(
            "refresh_idea", 
            {
                "research_goal": research_goal,
                "current_idea": main_idea, 
                "action_type": "execute"
            }
        )
        
        # Extract the idea content from the response
        new_idea = response.get("content", "")
        
        # Create a new state with trajectory-level memory
        # Start with depth 1 since this is directly connected to the root
        subject_from_state = getattr(current_node.state, "subject", None) if current_node else selected_subject
        new_state = MCTSState(
            research_goal=research_goal,
            current_idea=new_idea,
            retrieved_knowledge=[],  # Start with empty retrieved knowledge for new approach
            feedback={},  # Start with empty feedback for new approach
            depth=1,  # Directly connected to root, so depth is 1
            subject=subject_from_state
        )
        
        # Get review using the unified review method
        review_data = review_agent.unified_review(new_idea, subject=subject_from_state)
        avg_score = review_data.get("average_score") if review_data else None
        if avg_score is not None:
            print(f"Review score: {avg_score}")
        else:
            logger.warning(f"Review returned None average_score for refresh_idea route (subject: {subject_from_state})")
        if review_data:
            if "scores" in review_data:
                new_state.review_scores = review_data["scores"]
            if "reviews" in review_data:
                new_state.review_feedback = review_data["reviews"]
            if avg_score is not None:
                new_state.average_score = avg_score
                new_state.reward = avg_score
            else:
                new_state.average_score = 0.0
                new_state.reward = 0.0
        
        # Create new node and add as child of the ROOT node instead of current node
        new_node = current_root.add_child(new_state, "refresh_idea")
        
        # Update current node to the newly created node
        current_node = new_node
        
        # Update main idea
        main_idea = new_idea

        # Add completion message to chat
        chat_messages.append({"role": "system", "content": "Created a new approach based on the original research goal."})

        return jsonify({
            "idea": new_idea,
            "messages": chat_messages,
            "review_scores": getattr(new_state, "review_scores", {}),
            "average_score": getattr(new_state, "average_score", 0.0),
            "feedback": get_latest_feedback(getattr(new_state, "feedback", {})),
        })
        
    except Exception as e:
        error_message = f"Error refreshing idea: {str(e)}"
        chat_messages.append({"role": "system", "content": error_message})
        return jsonify({"error": error_message, "messages": chat_messages}), 500

# WebSocket endpoints for real-time MCTS exploration
@socketio.on('start_exploration')
def handle_start_exploration():
    global exploration_in_progress, current_state
    
    if exploration_in_progress:
        return {'error': 'Exploration already in progress'}
    
    exploration_in_progress = True
    
    try:
        # Create initial state if none exists
        if not current_state:
            current_state = MCTSState()
        
        # Start MCTS exploration with callback for updates
        def exploration_callback(message):
            emit('exploration_update', {
                'type': 'progress',
                'message': message
            })
        
        # Run MCTS with callback
        root = mcts.run(current_state, num_iterations=5, callback=exploration_callback)
        
        # Store best path
        best_node = root
        while best_node.children:
            best_node = max(best_node.children, key=lambda n: n.value)
            
        # Update current state
        current_state = best_node.state
        
        # Send final results
        emit('exploration_complete', {
            'idea': current_state.current_idea,
            'score': current_state.average_score if hasattr(current_state, 'average_score') else None,
            'tree_data': root.to_dict()
        })
        
    except Exception as e:
        emit('exploration_error', {'error': str(e)})
    finally:
        exploration_in_progress = False

@socketio.on('stop_exploration')
def handle_stop_exploration():
    global exploration_in_progress
    exploration_in_progress = False
    emit('exploration_stopped')

# Global state for selected topics
@app.route("/api/physics/topics", methods=["GET"])
def get_physics_topics():
    """Get all Physics topics from syllabus."""
    try:
        topics = load_physics_topics()
        return jsonify({"topics": topics})
    except Exception as e:
        logger.error(f"Error loading physics topics: {str(e)}")
        return jsonify({"error": f"Failed to load topics: {str(e)}"}), 500


@app.route("/api/chemistry/topics", methods=["GET"])
def get_chemistry_topics():
    """Get all Chemistry topics from syllabus."""
    try:
        topics = load_chemistry_topics()
        return jsonify({"topics": topics})
    except Exception as e:
        logger.error(f"Error loading chemistry topics: {str(e)}")
        return jsonify({"error": f"Failed to load topics: {str(e)}"}), 500


def parse_rq_candidates(content: str):
    """Parse multiple RQ candidates from model output."""
    if not content:
        return []
    candidates = []
    lines = [line.strip() for line in content.splitlines() if line.strip()]
    for line in lines:
        match = re.match(r"^\s*\d+[\).:\-]\s*(.+)$", line)
        if match:
            candidates.append(match.group(1).strip())
    if not candidates:
        inline_matches = re.findall(r"\d+[\).:\-]\s*([^\n]+)", content)
        candidates = [m.strip() for m in inline_matches if m.strip()]
    if not candidates and content.strip():
        candidates = [content.strip()]
    # Deduplicate while preserving order
    seen = set()
    unique = []
    for item in candidates:
        if item and item not in seen:
            seen.add(item)
            unique.append(item)
    return unique[:2]

@app.route("/api/generate_rq", methods=["POST"])
def generate_rq():
    """Generate hyper-specific RQ from IA topic."""
    data = request.get_json()
    
    if not data or "ia_topic" not in data:
        return jsonify({"error": "Missing ia_topic in request"}), 400
    
    ia_topic = data["ia_topic"]
    topics = data.get("topics", [])  # Get topics from request
    
    try:
        global current_node
        if not current_node:
            return jsonify({"error": "No current node found"}), 400
        
        # If topics not provided and subject is physics, load all topics from syllabus
        subject = getattr(current_node.state, "subject", None) or selected_subject
        if not topics and subject == "physics":
            try:
                topics = load_physics_topics()
            except Exception as e:
                logger.warning(f"Could not load physics topics: {e}")
                topics = []
        
        # Update selected topics in state if provided
        if topics:
            current_node.state.selected_topics = topics
        
        # Generate RQ using ideation agent
        response = ideation_agent.execute_action(
            "generate_rq",
            {
                "ia_topic": ia_topic,
                "current_state": current_node.state,
                "subject": subject,
                "topics": topics  # Pass all topics from syllabus to the agent
            }
        )
        
        content = response.get("content", "").strip()
        candidates = parse_rq_candidates(content)
        if not candidates:
            return jsonify({"error": "No research question was generated."}), 500

        rq_results = []
        for rq in candidates:
            is_valid = True
            warnings = []
            try:
                is_valid, warnings, _ = validate_rq_format(rq, subject or "physics", "ia")
            except Exception as validation_error:
                logger.warning(f"RQ validation error (non-fatal): {str(validation_error)}")
                warnings = ["Validation check encountered an error, but RQ was generated successfully."]
            rq_results.append({
                "text": rq,
                "is_valid": is_valid,
                "warnings": warnings
            })

        # Update state only when a single RQ is returned (approval sets it otherwise)
        if len(rq_results) == 1:
            current_node.state.research_question = rq_results[0]["text"]
        current_node.state.ia_topic = ia_topic
        
        primary = rq_results[0] if rq_results else {"text": "", "is_valid": True, "warnings": []}
        return jsonify({
            "research_questions": rq_results,
            "research_question": primary["text"],
            "is_valid": primary["is_valid"],
            "warnings": primary["warnings"]
        })
    
    except Exception as e:
        error_trace = traceback.format_exc()
        logger.error(f"RQ generation error: {str(e)}\n{error_trace}")
        return jsonify({"error": f"Failed to generate RQ: {str(e)}"}), 500

@app.route("/api/approve_rq", methods=["POST"])
def approve_rq():
    """Approve a research question and save it to state."""
    global current_node
    data = request.get_json()
    
    if not data or "research_question" not in data:
        return jsonify({"error": "Missing research_question in request"}), 400
    
    rq = data["research_question"]
    
    try:
        if current_node:
            current_node.state.research_question = rq
            return jsonify({"message": "Research Question approved", "research_question": rq})
        else:
            return jsonify({"error": "No current node found"}), 400
    except Exception as e:
        error_trace = traceback.format_exc()
        logger.error(f"Error approving RQ: {str(e)}\n{error_trace}")
        return jsonify({"error": f"Failed to approve RQ: {str(e)}"}), 500

@app.route("/api/approve_section/<section>", methods=["POST"])
def approve_section(section):
    """Approve a section (background, procedure, or research_design) and save it to state."""
    global current_node
    data = request.get_json()
    
    if not data or "content" not in data:
        return jsonify({"error": "Missing content in request"}), 400
    
    content = data["content"]
    citations = data.get("citations", [])
    
    try:
        if current_node:
            # Store section content in state
            if section == "background":
                current_node.state.background_content = content
                current_node.state.background_citations = citations
            elif section == "procedure":
                current_node.state.procedure_content = content
                current_node.state.procedure_citations = citations
            elif section == "research_design":
                current_node.state.research_design_content = content
                current_node.state.research_design_citations = citations
            
            return jsonify({"message": f"{section} approved", "content": content})
        else:
            return jsonify({"error": "No current node found"}), 400
    except Exception as e:
        error_trace = traceback.format_exc()
        logger.error(f"Error approving section {section}: {str(e)}\n{error_trace}")
        return jsonify({"error": f"Failed to approve section: {str(e)}"}), 500

def generate_citation_query(section_type: str, ia_topic: str, rq: str) -> str:
    """Generate ScholarQA query for citation retrieval."""
    if section_type == "background":
        return f"Background information and theoretical context for {ia_topic} related to {rq}"
    elif section_type == "procedure":
        return f"Experimental procedures and methods for {ia_topic} investigation"
    elif section_type == "research_design":
        return f"Research design, equipment specifications, and variable control for {ia_topic}"
    else:
        return f"Relevant research papers for {ia_topic} and {rq}"

def normalize_author(authors):
    """Normalize authors list/string into a single author reference."""
    if isinstance(authors, list):
        names = []
        for author in authors:
            if isinstance(author, dict):
                name = author.get("name", "")
            elif hasattr(author, "name"):
                name = author.name
            else:
                name = str(author)
            if name:
                names.append(name)
        if not names:
            return "Unknown"
        return f"{names[0]} et al." if len(names) > 1 else names[0]
    if isinstance(authors, str):
        return authors
    return "Unknown"

def extract_paper_url(citation_dict, paper):
    """Extract the best available URL for a paper."""
    # 1. Prefer open access PDF
    if paper.get("openAccessPdf") and paper["openAccessPdf"].get("url"):
        return paper["openAccessPdf"]["url"]
    
    # 2. Prefer paper URL from metadata (canonical)
    # Skip if it matches the broken short format https://www.semanticscholar.org/paper/12345
    url = paper.get("url")
    if url and not re.search(r'semanticscholar\.org/paper/\d+$', url):
        return url
    
    # 3. Prefer explicit citation URL if provided
    cite_url = citation_dict.get("url") if isinstance(citation_dict, dict) else None
    if cite_url and not re.search(r'semanticscholar\.org/paper/\d+$', cite_url):
        return cite_url
    
    # 4. Try paperId (hash format)
    paper_id = paper.get("paperId") or paper.get("paper_id")
    if not paper_id and isinstance(citation_dict, dict):
        paper_id = citation_dict.get("paperId") or citation_dict.get("paper_id")
    
    # Only use /paper/ path for hexadecimal hashes (not purely numeric)
    if paper_id and not str(paper_id).isdigit():
        return f"https://www.semanticscholar.org/paper/{paper_id}"
    
    # 5. Fallback to official API redirect service for numeric IDs
    # This is what works in the literature panel
    cid = paper.get("corpus_id") or paper.get("corpusId") or (paper_id if paper_id and str(paper_id).isdigit() else None)
    if cid:
        numeric_id = str(cid).replace("CorpusID:", "").strip()
        return f"https://api.semanticscholar.org/CorpusID:{numeric_id}"
    
    return ""

def retrieve_section_knowledge(section: str, ia_topic: str, research_question: str, additional_context: str = ""):
    """Retrieve section-specific knowledge and citations from ScholarQA."""
    query = generate_citation_query(section, ia_topic, research_question)
    if additional_context:
        query = f"{query} {additional_context}"

    try:
        result = scholar_qa.answer_query(query)
    except Exception as e:
        logger.error(f"Error retrieving knowledge for {section}: {str(e)}")
        return "", []

    sections = result.get("sections", []) if isinstance(result, dict) else getattr(result, "sections", [])
    retrieved_content = []
    citations_list = []

    for sec in sections:
        if isinstance(sec, dict):
            section_dict = sec
        else:
            section_dict = sec.model_dump() if hasattr(sec, "model_dump") else sec.dict() if hasattr(sec, "dict") else {}

        section_text = section_dict.get("text", "")
        if section_text:
            title = section_dict.get("title", "Untitled")
            retrieved_content.append(f"## {title}\n\n{section_text}")

        for citation in section_dict.get("citations", []):
            if isinstance(citation, dict):
                citation_dict = citation
            else:
                citation_dict = citation.model_dump() if hasattr(citation, "model_dump") else citation.dict() if hasattr(citation, "dict") else {}

            paper = citation_dict.get("paper", {})
            if not isinstance(paper, dict):
                paper = paper.model_dump() if hasattr(paper, "model_dump") else paper.dict() if hasattr(paper, "dict") else {}

            author = normalize_author(paper.get("authors"))
            year = paper.get("year")

            if not author or author in ("N/A", "Unknown") or not year or year == "N/A":
                continue

            paper_url = extract_paper_url(citation_dict, paper)

            citations_list.append({
                "id": citation_dict.get("id", ""),
                "author": author,
                "year": year,
                "title": paper.get("title", "Untitled"),
                "corpus_id": paper.get("corpus_id", ""),
                "citation_count": paper.get("citation_count", 0),
                "url": paper_url
            })

    return "\n\n".join(retrieved_content), citations_list[:10]

def retrieve_citations_for_section(query: str):
    """Retrieve citations using ScholarQA and format them."""
    try:
        result = scholar_qa.answer_query(query)
        citations = []
        
        # Extract citations from ScholarQA result
        sections = result.get("sections", [])
        for section in sections:
            section_citations = section.get("citations", [])
            for citation in section_citations:
                if isinstance(citation, dict):
                    paper = citation.get("paper", {})
                    citation_id = citation.get("id", "")
                    
                    # Validate paper metadata to avoid N/A hallucinations
                    author = normalize_author(paper.get("authors"))
                    year = paper.get("year")
                    
                    if not author or author in ("N/A", "Unknown") or not year or year == "N/A":
                        continue
                        
                    paper_url = extract_paper_url(citation, paper)
                    
                    citations.append({
                        "id": citation_id,
                        "author": author,
                        "year": year,
                        "title": paper.get("title", "Untitled"),
                        "corpus_id": paper.get("corpus_id", ""),
                        "citation_count": paper.get("citation_count", 0),
                        "url": paper_url
                    })
        
        return citations[:10]  # Limit to 10 citations
    except Exception as e:
        logger.error(f"Error retrieving citations: {str(e)}")
        return []

def format_citation_for_prompt(citation_dict):
    """Format citation dictionary to [ID | AUTHOR_REF | YEAR | Citations: CITES] format."""
    author = citation_dict.get("author", "Unknown")
    year = citation_dict.get("year", "")
    citation_id = citation_dict.get("id", citation_dict.get("corpus_id", ""))
    citation_count = citation_dict.get("citation_count", 0)
    
    return f"[{citation_id} | {author} | {year} | Citations: {citation_count}]"

def format_citation_for_prompt_numbered(citation_dict, number: int):
    """Format citation dictionary to numbered references for inline [n] use."""
    author = citation_dict.get("author", "Unknown")
    year = citation_dict.get("year", "")
    title = citation_dict.get("title", "Untitled")
    url = citation_dict.get("url", "")
    url_part = f" | {url}" if url else ""
    return f"{number}. {author} ({year}) - {title}{url_part}"

@app.route("/api/expand/background", methods=["POST"])
def expand_background():
    """Expand Background Information section with citations."""
    data = request.get_json()
    
    if not data or "ia_topic" not in data or "research_question" not in data:
        return jsonify({"error": "Missing required fields"}), 400
    
    ia_topic = data["ia_topic"]
    research_question = data["research_question"]
    feedback = data.get("feedback")
    previous_content = data.get("previous_content")
    auto_retrieve = data.get("auto_retrieve", True)
    
    try:
        global current_node
        if not current_node:
            return jsonify({"error": "No current node found"}), 400
        
        retrieved_knowledge = ""
        if auto_retrieve:
            retrieved_knowledge, citations_list = retrieve_section_knowledge("background", ia_topic, research_question)
        else:
            query = generate_citation_query("background", ia_topic, research_question)
            citations_list = retrieve_citations_for_section(query)
        citations_str = "\n".join([format_citation_for_prompt_numbered(c, i + 1) for i, c in enumerate(citations_list)])
        
        # Get research brief content
        research_brief = main_idea or (current_node.state.content if hasattr(current_node.state, 'content') else "")
        
        # Include feedback in prompt if provided
        state_dict = {
            "ia_topic": ia_topic,
            "research_question": research_question,
            "research_brief": research_brief,
            "citations": citations_str,
            "retrieved_knowledge": retrieved_knowledge,
            "current_state": current_node.state,
            "subject": current_node.state.subject
        }
        if feedback:
            state_dict["feedback"] = feedback
        if previous_content:
            state_dict["previous_content"] = previous_content
        
        # Generate section
        response = ideation_agent.execute_action(
            "expand_background",
            state_dict
        )
        
        content = response.get("content", "")
        
        # Update state
        if not current_node.state.expanded_sections:
            current_node.state.expanded_sections = {}
        current_node.state.expanded_sections["background"] = content
        current_node.state.section_citations["background"] = citations_list
        
        return jsonify({
            "content": content,
            "citations": citations_list,
            "retrieved_knowledge": retrieved_knowledge
        })
    
    except Exception as e:
        error_trace = traceback.format_exc()
        logger.error(f"Background expansion error: {str(e)}\n{error_trace}")
        return jsonify({"error": f"Failed to expand background: {str(e)}"}), 500

@app.route("/api/expand/procedure", methods=["POST"])
def expand_procedure():
    """Expand Procedure section with citations."""
    data = request.get_json()
    
    if not data or "ia_topic" not in data or "research_question" not in data:
        return jsonify({"error": "Missing required fields"}), 400
    
    ia_topic = data["ia_topic"]
    research_question = data["research_question"]
    feedback = data.get("feedback")
    previous_content = data.get("previous_content")
    auto_retrieve = data.get("auto_retrieve", True)
    
    try:
        global current_node
        if not current_node:
            return jsonify({"error": "No current node found"}), 400
        
        retrieved_knowledge = ""
        if auto_retrieve:
            retrieved_knowledge, citations_list = retrieve_section_knowledge("procedure", ia_topic, research_question)
        else:
            query = generate_citation_query("procedure", ia_topic, research_question)
            citations_list = retrieve_citations_for_section(query)
        citations_str = "\n".join([format_citation_for_prompt(c) for c in citations_list])
        
        # Get research brief content
        research_brief = main_idea or (current_node.state.content if hasattr(current_node.state, 'content') else "")
        
        # Generate section
        # Include feedback in prompt if provided
        state_dict = {
            "ia_topic": ia_topic,
            "research_question": research_question,
            "research_brief": research_brief,
            "citations": citations_str,
            "retrieved_knowledge": retrieved_knowledge,
            "current_state": current_node.state,
            "subject": current_node.state.subject
        }
        if feedback:
            state_dict["feedback"] = feedback
        if previous_content:
            state_dict["previous_content"] = previous_content
        
        response = ideation_agent.execute_action(
            "expand_procedure",
            state_dict
        )
        
        content = response.get("content", "")
        
        # Update state
        if not current_node.state.expanded_sections:
            current_node.state.expanded_sections = {}
        current_node.state.expanded_sections["procedure"] = content
        current_node.state.section_citations["procedure"] = citations_list
        
        return jsonify({
            "content": content,
            "citations": citations_list,
            "retrieved_knowledge": retrieved_knowledge
        })
    
    except Exception as e:
        error_trace = traceback.format_exc()
        logger.error(f"Procedure expansion error: {str(e)}\n{error_trace}")
        return jsonify({"error": f"Failed to expand procedure: {str(e)}"}), 500

@app.route("/api/expand/research_design", methods=["POST"])
def expand_research_design():
    """Expand Research Design section with citations."""
    data = request.get_json()
    
    if not data or "ia_topic" not in data or "research_question" not in data:
        return jsonify({"error": "Missing required fields"}), 400
    
    ia_topic = data["ia_topic"]
    research_question = data["research_question"]
    feedback = data.get("feedback")
    previous_content = data.get("previous_content")
    auto_retrieve = data.get("auto_retrieve", True)
    
    try:
        global current_node
        if not current_node:
            return jsonify({"error": "No current node found"}), 400
        
        retrieved_knowledge = ""
        # Research Design doesn't need citations
        citations_list = []
        citations_str = ""
        
        # Get research brief content
        research_brief = main_idea or (current_node.state.content if hasattr(current_node.state, 'content') else "")
        
        # Generate section
        # Include feedback in prompt if provided
        state_dict = {
            "ia_topic": ia_topic,
            "research_question": research_question,
            "research_brief": research_brief,
            "citations": citations_str,
            "retrieved_knowledge": retrieved_knowledge,
            "current_state": current_node.state,
            "subject": current_node.state.subject
        }
        if feedback:
            state_dict["feedback"] = feedback
        if previous_content:
            state_dict["previous_content"] = previous_content
        
        response = ideation_agent.execute_action(
            "expand_research_design",
            state_dict
        )
        
        content = response.get("content", "")
        
        # Update state
        if not current_node.state.expanded_sections:
            current_node.state.expanded_sections = {}
        current_node.state.expanded_sections["research_design"] = content
        # Research Design doesn't need citations, so don't store them
        if not hasattr(current_node.state, 'section_citations'):
            current_node.state.section_citations = {}
        current_node.state.section_citations["research_design"] = []
        
        return jsonify({
            "content": content,
            "citations": citations_list,
            "retrieved_knowledge": retrieved_knowledge
        })
    
    except Exception as e:
        error_trace = traceback.format_exc()
        logger.error(f"Research design expansion error: {str(e)}\n{error_trace}")
        return jsonify({"error": f"Failed to expand research design: {str(e)}"}), 500

@app.route("/api/citations/add", methods=["POST"])
def add_citation():
    """Manually add citation to a section."""
    data = request.get_json()
    
    if not data or "section" not in data or "citation" not in data:
        return jsonify({"error": "Missing section or citation"}), 400
    
    section = data["section"]
    citation = data["citation"]
    
    try:
        global current_node
        if not current_node:
            return jsonify({"error": "No current node found"}), 400
        
        if not current_node.state.section_citations:
            current_node.state.section_citations = {}
        if section not in current_node.state.section_citations:
            current_node.state.section_citations[section] = []
        
        current_node.state.section_citations[section].append(citation)
        
        return jsonify({"success": True, "citations": current_node.state.section_citations[section]})
    
    except Exception as e:
        logger.error(f"Error adding citation: {str(e)}")
        return jsonify({"error": f"Failed to add citation: {str(e)}"}), 500

@app.route("/api/citations/retrieve", methods=["POST"])
def retrieve_citations():
    """Auto-retrieve citations for a section using ScholarQA."""
    data = request.get_json()
    
    if not data or "section" not in data or "ia_topic" not in data or "research_question" not in data:
        return jsonify({"error": "Missing required fields"}), 400
    
    section = data["section"]
    ia_topic = data["ia_topic"]
    research_question = data["research_question"]
    
    # Research Design doesn't need citations
    if section == "research_design":
        return jsonify({"citations": []})
    
    try:
        query = generate_citation_query(section, ia_topic, research_question)
        citations_list = retrieve_citations_for_section(query)
        
        return jsonify({"citations": citations_list})
    
    except Exception as e:
        logger.error(f"Error retrieving citations: {str(e)}")
        return jsonify({"error": f"Failed to retrieve citations: {str(e)}"}), 500


@app.route("/api/section/improve_with_knowledge", methods=["POST"])
def improve_section_with_knowledge():
    """Improve a section (background, procedure, research_design) with retrieved knowledge."""
    global current_node
    
    data = request.get_json()
    if not data or "section" not in data:
        return jsonify({"error": "Missing section in request"}), 400
    
    section = data["section"]
    ia_topic = data.get("ia_topic", "")
    research_question = data.get("research_question", "")
    current_content = data.get("current_content", "")
    additional_context = data.get("additional_context", "")
    
    if not ia_topic and current_node:
        ia_topic = current_node.state.ia_topic if hasattr(current_node.state, 'ia_topic') else ""
    if not research_question and current_node:
        research_question = current_node.state.research_question if hasattr(current_node.state, 'research_question') else ""
    
    try:
        # Research Design doesn't need citations
        if section == "research_design":
            retrieved_content = []
            citations_list = []
            formatted_knowledge = ""
            citations_str = ""
        else:
            # First retrieve relevant papers using ScholarQA
            query = generate_citation_query(section, ia_topic, research_question)
            # Add user's additional context to improve search
            if additional_context:
                query = f"{query} {additional_context}"
            
            # Use the full ScholarQA to get paper content, not just citations
            result = scholar_qa.answer_query(query)
            
            # Format retrieved knowledge
            retrieved_content = []
            citations_list = []
            
            sections = result.get("sections", [])
            for sec in sections:
                section_text = f"## {sec.get('title', 'Untitled')}\n\n"
                section_text += f"{sec.get('text', '')}\n\n"
                retrieved_content.append(section_text)
                
                # Extract citations
                for citation in sec.get("citations", []):
                    if isinstance(citation, dict):
                        paper = citation.get("paper", {})
                        citation_id = citation.get("id", "")
                        
                        # Validate paper metadata to avoid N/A hallucinations
                        author = normalize_author(paper.get("authors"))
                        year = paper.get("year")
                        
                        if not author or author in ("N/A", "Unknown") or not year or year == "N/A":
                            continue
                            
                        paper_url = extract_paper_url(citation, paper)
                        
                        citations_list.append({
                            "id": citation_id,
                            "author": author,
                            "year": year,
                            "title": paper.get("title", "Untitled"),
                            "corpus_id": paper.get("corpus_id", ""),
                            "citation_count": paper.get("citation_count", 0),
                            "url": paper_url
                        })
            
            formatted_knowledge = "\n\n".join(retrieved_content)
            if section == "background":
                citations_str = "\n".join([format_citation_for_prompt_numbered(c, i + 1) for i, c in enumerate(citations_list[:10])])
            else:
                citations_str = "\n".join([format_citation_for_prompt(c) for c in citations_list[:10]])
        
        # Get research brief content
        research_brief = main_idea or (current_node.state.content if current_node and hasattr(current_node.state, 'content') else "")
        
        # Prepare state for section regeneration with knowledge
        state_dict = {
            "ia_topic": ia_topic,
            "research_question": research_question,
            "research_brief": research_brief,
            "citations": citations_str,
            "retrieved_knowledge": formatted_knowledge,
            "previous_content": current_content,
            "current_state": current_node.state if current_node else None,
            "subject": current_node.state.subject if current_node else "physics"
        }
        
        # Map section to action
        action_map = {
            "background": "expand_background",
            "procedure": "expand_procedure",
            "research_design": "expand_research_design"
        }
        
        action = action_map.get(section, "expand_background")
        
        # Generate improved section with real citations
        response = ideation_agent.execute_action(action, state_dict)
        content = response.get("content", "")
        
        # Update state
        if current_node and not current_node.state.expanded_sections:
            current_node.state.expanded_sections = {}
        if current_node:
            current_node.state.expanded_sections[section] = content
            # Research Design doesn't need citations
            if section != "research_design":
                if not hasattr(current_node.state, 'section_citations'):
                    current_node.state.section_citations = {}
                current_node.state.section_citations[section] = citations_list[:10]
            else:
                if not hasattr(current_node.state, 'section_citations'):
                    current_node.state.section_citations = {}
                current_node.state.section_citations[section] = []
        
        return jsonify({
            "content": content,
            "citations": citations_list[:10],
            "retrieved_knowledge": formatted_knowledge[:1000] + "..." if len(formatted_knowledge) > 1000 else formatted_knowledge
        })
    
    except Exception as e:
        error_trace = traceback.format_exc()
        logger.error(f"Error improving section with knowledge: {str(e)}\n{error_trace}")
        return jsonify({"error": f"Failed to improve section: {str(e)}"}), 500


@app.route("/api/set_aspect_weights", methods=["POST"])
def set_aspect_weights():
    """Update the weights for different review aspects."""
    try:
        data = request.get_json()
        if not data or "weights" not in data:
            return jsonify({"error": "Missing weights"}), 400
            
        weights = data["weights"]
        # Validate that we have all required aspects
        required_aspects = ["novelty", "clarity", "feasibility", "effectiveness", "impact"]
        if not all(aspect in weights for aspect in required_aspects):
            return jsonify({"error": "Missing required aspects"}), 400
            
        # Update weights in review agent
        review_agent.set_aspect_weights(weights)
        
        return jsonify({
            "success": True,
            "message": "Aspect weights updated successfully"
        })
        
    except Exception as e:
        error_message = f"Error setting aspect weights: {str(e)}"
        print(error_message)
        return jsonify({"error": error_message}), 500

@app.cli.command("check-config")
def check_configuration():
    """Check the application configuration and environment."""
    issues = []
    warnings = []
    
    # Check Flask configuration
    if not os.environ.get('FLASK_SECRET_KEY'):
        if os.environ.get('FLASK_ENV') == 'production':
            issues.append("FLASK_SECRET_KEY must be set in production")
        else:
            warnings.append("FLASK_SECRET_KEY not set - using default (unsafe for production)")
            
    # Check required API keys
    required_keys = {
        'GEMINI_API_KEY': 'Google Gemini',
        'HUGGINGFACE_API_KEY': 'HuggingFace',
        'SEMANTIC_SCHOLAR_API_KEY': 'Semantic Scholar'
    }
    
    for env_var, provider in required_keys.items():
        if not os.environ.get(env_var):
            issues.append(f"Missing required {provider} API key ({env_var})")
            
    # Check optional API keys
    optional_keys = {
        'OPENAI_API_KEY': 'OpenAI',
        'ANTHROPIC_API_KEY': 'Anthropic/Claude',
        'DEEPSEEK_API_KEY': 'DeepSeek'
    }
    
    for env_var, provider in optional_keys.items():
        if not os.environ.get(env_var):
            warnings.append(f"Optional {provider} API key not configured ({env_var})")
            
    # Check secure directories
    if not os.path.exists(SECURE_KEYS_DIR):
        issues.append(f"Secure keys directory {SECURE_KEYS_DIR} does not exist")
    elif os.environ.get('FLASK_ENV') == 'production':
        # Check directory permissions in production
        import stat
        key_dir_stat = os.stat(SECURE_KEYS_DIR)
        if key_dir_stat.st_mode & stat.S_IROTH or key_dir_stat.st_mode & stat.S_IWOTH:
            issues.append(f"Secure keys directory {SECURE_KEYS_DIR} has unsafe permissions")
            
    # Check upload directory
    if not os.path.exists(app.config["UPLOAD_FOLDER"]):
        issues.append(f"Upload directory {app.config['UPLOAD_FOLDER']} does not exist")
        
    # Print results
    if issues:
        click.secho("\nâŒ Configuration issues found:", fg='red', bold=True)
        for issue in issues:
            click.secho(f"  - {issue}", fg='red')
            
    if warnings:
        click.secho("\nâš ï¸  Configuration warnings:", fg='yellow', bold=True)
        for warning in warnings:
            click.secho(f"  - {warning}", fg='yellow')
            
    if not issues and not warnings:
        click.secho("\nâœ… Configuration valid!", fg='green', bold=True)
    elif not issues:
        click.secho("\nâœ… No critical issues found (see warnings above)", fg='green')
        
    return len(issues) == 0

if __name__ == "__main__":
    # Railway uses PORT, fallback to FLASK_RUN_PORT for local dev
    port = int(os.environ.get('PORT', os.environ.get('FLASK_RUN_PORT', config.get('app', {}).get('port', 5001))))
    socketio.run(app, host='0.0.0.0', port=port, debug=True, allow_unsafe_werkzeug=True)
